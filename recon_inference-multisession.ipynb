{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16c9d4c-66cb-4692-a61d-9aa86a8765d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing modules\n",
      "SLURM random seed indices not provided; using random seed = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"importing modules\")\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "import utils\n",
    "from utils import load_preprocess_betas, resample, applyxfm, apply_thresh, resample_betas\n",
    "\n",
    "import importlib.util\n",
    "parent_utils_path = \"/home/ri4541/mindeye_preproc/analysis/utils.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"utils\", parent_utils_path)\n",
    "preproc = importlib.util.module_from_spec(spec)\n",
    "parent_dir = os.path.dirname(parent_utils_path)  # Extract directory\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "spec.loader.exec_module(preproc)\n",
    "\n",
    "# Can run a SLURM job array to train many models with different random seed values\n",
    "try:\n",
    "    seed = int(os.environ[\"SLURM_ARRAY_TASK_ID\"])\n",
    "    print(f\"using random seed {seed} in SLURM job {seed} of array\")\n",
    "except:\n",
    "    print(\"SLURM random seed indices not provided; using random seed = 0\")\n",
    "    seed = 0\n",
    "\n",
    "if utils.is_interactive():\n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9ed4e331-4376-445e-b35a-a917baf0daea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "accelerator = Accelerator(split_batches=False)\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d8de1-d0ca-4b5f-84d8-2560f0399a5a",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c47b5b-869f-468c-bb93-43610ee5dbe0",
   "metadata": {},
   "source": [
    "## New Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69037852-cdbd-4eac-a720-3fca5dc48a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if utils.is_interactive():\n",
    "    sub = \"sub-005\"\n",
    "    session = \"ses-01\"\n",
    "    task = 'C'  # 'study' or 'A'; used to search for functional run in bids format\n",
    "else:\n",
    "    sub = os.environ[\"sub\"]\n",
    "    session = os.environ[\"session\"]\n",
    "    task = os.environ[\"task\"]\n",
    "\n",
    "if session == \"all\":\n",
    "    ses_list = [\"ses-01\", \"ses-02\"]  # list of actual session IDs\n",
    "    design_ses_list = [\"ses-01\", \"ses-02\"]  # list of session IDs to search for design matrix\n",
    "else:\n",
    "    ses_list = [session]\n",
    "    design_ses_list = [session]\n",
    "    \n",
    "task_name = f\"_task-{task}\" if task != 'study' else ''\n",
    "resample_voxel_size = False\n",
    "resample_post_glmsingle = False  # do you want to do voxel resampling here? if resample_voxel_size = True and resample_post_glmsingle = False, assume the resampling has been done prior to GLMsingle, so just use resampled directory but otherwise proceed as normal\n",
    "load_from_resampled_file = False  # do you want to load resampled data from file? if True, assume resampling was done in this notebook before, and that we're not using the GLMsingle resampled data\n",
    "    \n",
    "train_test_split = 'MST' # 'MST', 'orig', 'unique'\n",
    "remove_close_to_MST = False\n",
    "remove_random_n = False\n",
    "\n",
    "if remove_close_to_MST or remove_random_n:\n",
    "    assert remove_close_to_MST != remove_random_n  # don't remove both sets of images\n",
    "\n",
    "n_to_remove = 0\n",
    "if remove_random_n:\n",
    "    assert train_test_split == 'MST'  # MST images are excluded from the n images removed, so only makes sense if they're not in the training set\n",
    "    n_to_remove = 150\n",
    "    \n",
    "if resample_voxel_size:\n",
    "    # voxel size was unchanged in glmsingle, want to perform resampling here\n",
    "    resampled_vox_size = 2.5\n",
    "    resample_method = \"sinc\"  # {trilinear,nearestneighbour,sinc,spline}, credit: https://johnmuschelli.com/fslr/reference/flirt.help.html\n",
    "    \n",
    "    # file name helper variables\n",
    "    vox_dim_str = str(resampled_vox_size).replace('.', '_')  # in case the voxel size has a decimal, replace with an underscore\n",
    "    resampled_suffix = f\"resampled_{vox_dim_str}mm_{resample_method}\"\n",
    "    mask_resampled_suffix = resampled_suffix\n",
    "    if resample_post_glmsingle:\n",
    "        resampled_suffix += '_postglmsingle'\n",
    "    else:\n",
    "        resampled_suffix += '_preglmsingle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ece766e-4272-4ca3-81e9-9ea5dccd2279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session label: ses-01\n"
     ]
    }
   ],
   "source": [
    "session_label = preproc.get_session_label(ses_list)\n",
    "print('session label:', session_label)\n",
    "n_runs, _ = preproc.get_runs_per_session(sub, session, ses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e52985b1-95ff-487b-8b2d-cc1ad1c190b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: sub-005_ses-01_task-C_bs24_MST_rishab_MSTsplit\n",
      "glmsingle_path: /scratch/gpfs/ri4541/MindEyeV2/src/mindeyev2/glmsingle_sub-005_ses-01_task-C\n",
      "--data_path=/scratch/gpfs/ri4541/MindEyeV2/src/mindeyev2                     --model_name=sub-005_ses-01_task-C_bs24_MST_rishab_MSTsplit --subj=1                     --no-blurry_recon --use_prior                     --hidden_dim=1024 --n_blocks=4\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name=f\"{sub}_{session}_task-{task}_bs24_MST_rishab_{train_test_split}split\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    glmsingle_path=f\"/scratch/gpfs/ri4541/MindEyeV2/src/mindeyev2/glmsingle_{sub}_{session}_task-{task}\"\n",
    "    print(\"glmsingle_path:\", glmsingle_path)\n",
    "    # global_batch_size and batch_size should already be defined in the above cells\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=/scratch/gpfs/ri4541/MindEyeV2/src/mindeyev2 \\\n",
    "                    --model_name={model_name} --subj=1 \\\n",
    "                    --no-blurry_recon --use_prior \\\n",
    "                    --hidden_dim=1024 --n_blocks=4\"\n",
    "    \n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e5dae4-606d-4dc6-b420-df9e4c14737e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"will load ckpt for model found in ../train_logs/model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/weka/proj-fmri/shared/mindeyev2_dataset\",\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=2048,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_len\",type=int,default=1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--glmsingle_path\",type=str,default=\"/scratch/gpfs/ri4541/MindEyeV2/src/mindeyev2/glmsingle_ses-01\",\n",
    ")\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# make output directory\n",
    "# os.makedirs(\"evals\",exist_ok=True)\n",
    "# os.makedirs(f\"evals/{model_name}\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c1e0c6-0641-4239-8201-f2c676532302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv/sub-005_ses-01.csv\n",
      "(790, 122)\n",
      "len_unique_images 532\n",
      "n_runs 11\n",
      "['all_stimuli/special515/special_15939.jpg'\n",
      " 'all_stimuli/special515/special_23241.jpg'\n",
      " 'all_stimuli/special515/special_32232.jpg'\n",
      " 'all_stimuli/special515/special_34238.jpg']\n",
      "[190.2773371 194.2907918 198.3011098 202.3095724]\n",
      "[0. 0. 0. 0.]\n",
      "(693,)\n"
     ]
    }
   ],
   "source": [
    "if session == \"all\":\n",
    "    filename = f\"csv/{sub}_{ses_list[0]}.csv\"\n",
    "    data = pd.read_csv(filename)[14:]\n",
    "    print(filename)\n",
    "    print(data.shape)\n",
    "    for s in ses_list[1:]:\n",
    "        filename = f\"csv/{sub}_{s}.csv\"\n",
    "        print(filename)\n",
    "        data = pd.concat([data, pd.read_csv(filename)[14:]])\n",
    "        print(data.shape)\n",
    "else:\n",
    "    filename = f\"csv/{sub}_{session}.csv\"\n",
    "    if sub == 'sub-001' and session == 'ses-01':\n",
    "        data = pd.read_csv(filename)[23:]\n",
    "    else: \n",
    "        data = pd.read_csv(filename)[14:]\n",
    "    print(filename)\n",
    "    print(data.shape)\n",
    "\n",
    "image_names = data['current_image'].values\n",
    "starts = data['trial.started'].values\n",
    "is_new_run = data['is_new_run'].values\n",
    "\n",
    "if sub == 'sub-001':\n",
    "    if session == 'ses-01':\n",
    "        assert image_names[0] == 'images/image_686_seed_1.png'\n",
    "    elif session in ('ses-02', 'all'):\n",
    "        assert image_names[0] == 'all_stimuli/special515/special_40840.jpg'\n",
    "    elif session == 'ses-03':\n",
    "        assert image_names[0] == 'all_stimuli/special515/special_69839.jpg'\n",
    "    elif session == 'ses-04':\n",
    "        assert image_names[0] == 'all_stimuli/rtmindeye_stimuli/image_686_seed_1.png'\n",
    "elif sub == 'sub-003':\n",
    "    assert image_names[0] == 'all_stimuli/rtmindeye_stimuli/image_686_seed_1.png'\n",
    "\n",
    "unique_images = np.unique(image_names.astype(str))\n",
    "unique_images = unique_images[(unique_images!=\"nan\")]\n",
    "# unique_images = unique_images[(unique_images!=\"blank.jpg\")]\n",
    "len_unique_images = len(unique_images)\n",
    "print(\"len_unique_images\",len_unique_images)\n",
    "print(\"n_runs\",n_runs)\n",
    "\n",
    "if (sub == 'sub-001' and session == 'ses-04') or (sub == 'sub-003' and session == 'ses-01'):\n",
    "    assert len(unique_images) == 851\n",
    "\n",
    "print(image_names[:4])\n",
    "print(starts[:4])\n",
    "print(is_new_run[:4])\n",
    "\n",
    "if remove_random_n:\n",
    "    # want to remove 150 imgs\n",
    "    # 100 special515 imgs are repeated 3x (300 total)\n",
    "    # all other train imgs are only shown once (558 total)\n",
    "    # of the 150, want to sample proportionally since we're cutting all repeats for special515\n",
    "    # so take out 51 (17 unique) from special515 and 99 from rest = removing 150 total\n",
    "    np.random.seed(seed)\n",
    "    options_to_remove = [x for x in set(image_names) if str(x) != 'nan' and x != 'blank.jpg' and 'MST_pairs' not in x and 'special515' not in x and list(image_names).count(x)==1]  # all the imgs that only appear once (this is O(N^2) b/c of count() within list comprehension but image_names is a relatively small list)\n",
    "    options_to_remove_special515 = [x for x in set(image_names) if str(x) != 'nan' and x != 'blank.jpg' and 'MST_pairs' not in x and 'special515' in x and list(image_names).count(x)>1]  # all the special515 images that are repeated (count()>1 necessary because there are special515 that are not repeated)\n",
    "    imgs_to_remove = np.random.choice(options_to_remove, size=99, replace=False)\n",
    "    imgs_to_remove = np.append(imgs_to_remove, np.random.choice(options_to_remove_special515, size=17, replace=False))\n",
    "\n",
    "image_idx = np.array([])  # contains the unique index of each presented image\n",
    "vox_image_names = np.array([])  # contains the names of the images corresponding to image_idx\n",
    "all_MST_images = dict()\n",
    "for i, im in enumerate(image_names):\n",
    "    # skip if blank, nan\n",
    "    if im == \"blank.jpg\":\n",
    "        i+=1\n",
    "        continue\n",
    "    if str(im) == \"nan\":\n",
    "        i+=1\n",
    "        continue\n",
    "    vox_image_names = np.append(vox_image_names, im)\n",
    "    if remove_close_to_MST:  # optionally skip close_to_MST images \n",
    "        if \"closest_pairs\" in im:\n",
    "            i+=1\n",
    "            continue\n",
    "    elif remove_random_n:\n",
    "        if im in imgs_to_remove:\n",
    "            i+=1\n",
    "            continue\n",
    "            \n",
    "    image_idx_ = np.where(im==unique_images)[0].item()\n",
    "    image_idx = np.append(image_idx, image_idx_)\n",
    "    \n",
    "    if (sub == 'sub-001' and session == 'ses-04') or (sub == 'sub-003' and session == 'ses-01'):  # MST images are ones that matched these image titles\n",
    "        import re\n",
    "        if ('w_' in im or 'paired_image_' in im or re.match(r'all_stimuli/rtmindeye_stimuli/\\d{1,2}_\\d{1,3}\\.png$', im) or re.match(r'images/\\d{1,2}_\\d{1,3}\\.png$', im)):  \n",
    "        # the regexp here looks for **_***.png, allows 1-2 chars before underscore and 1-3 chars after it\n",
    "            # print(im)\n",
    "            all_MST_images[i] = im\n",
    "            i+=1            \n",
    "    elif 'MST' in im:\n",
    "        all_MST_images[i] = im\n",
    "        i+=1\n",
    "    \n",
    "image_idx = torch.Tensor(image_idx).long()\n",
    "# for im in new_image_names[MST_images]:\n",
    "#     assert 'MST_pairs' in im\n",
    "# assert len(all_MST_images) == 300\n",
    "\n",
    "unique_MST_images = np.unique(list(all_MST_images.values())) \n",
    "\n",
    "MST_ID = np.array([], dtype=int)\n",
    "if remove_close_to_MST:\n",
    "    close_to_MST_idx = np.array([], dtype=int)\n",
    "if remove_random_n:\n",
    "    random_n_idx = np.array([], dtype=int)\n",
    "\n",
    "vox_idx = np.array([], dtype=int)\n",
    "j=0  # this is a counter keeping track of the remove_random_n used later to index vox based on the removed images; unused otherwise\n",
    "for i, im in enumerate(image_names):  # need unique_MST_images to be defined, so repeating the same loop structure\n",
    "    # skip if blank, nan\n",
    "    if im == \"blank.jpg\":\n",
    "        i+=1\n",
    "        continue\n",
    "    if str(im) == \"nan\":\n",
    "        i+=1\n",
    "        continue\n",
    "    if remove_close_to_MST:  # optionally skip close_to_MST images \n",
    "        if \"closest_pairs\" in im:\n",
    "            close_to_MST_idx = np.append(close_to_MST_idx, i)\n",
    "            i+=1\n",
    "            continue\n",
    "    if remove_random_n:\n",
    "        if im in imgs_to_remove:\n",
    "            vox_idx = np.append(vox_idx, j)\n",
    "            i+=1\n",
    "            j+=1\n",
    "            continue\n",
    "    j+=1\n",
    "    curr = np.where(im == unique_MST_images)\n",
    "    # print(curr)\n",
    "    if curr[0].size == 0:\n",
    "        MST_ID = np.append(MST_ID, np.array(len(unique_MST_images)))  # add a value that should be out of range based on the for loop, will index it out later\n",
    "    else:\n",
    "        MST_ID = np.append(MST_ID, curr)\n",
    "        \n",
    "assert len(MST_ID) == len(image_idx)\n",
    "# assert len(np.argwhere(pd.isna(data['current_image']))) + len(np.argwhere(data['current_image'] == 'blank.jpg')) + len(image_idx) == len(data)\n",
    "# MST_ID = torch.tensor(MST_ID[MST_ID != len(unique_MST_images)], dtype=torch.uint8)  # torch.tensor (lowercase) allows dtype kwarg, Tensor (uppercase) is an alias for torch.FloatTensor\n",
    "print(MST_ID.shape)\n",
    "if (sub == 'sub-001' and session == 'ses-04') or (sub == 'sub-003' and session == 'ses-01'):\n",
    "    assert len(all_MST_images) == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd08fa34-ebd0-482a-bc29-8fb32c8b888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_images_pairs = [\n",
    "#     (1,2),(3,4),(5,6),(7,8),(9,10),(11,12),(13,14),(15,16),\n",
    "#     (17,18),(19,20),(21,22),(23,24),(25,26),(27,28),(29,30),\n",
    "#     (31,32),(33,34),(35,36),\n",
    "#     (787, 788), (789, 790), (791, 792), (793, 794), (795, 796),\n",
    "#     (797, 798), (799, 800), (801, 802), (803, 804), (805, 806),\n",
    "#     (807, 808), (809, 810), (811, 812), (813, 814), (815, 816),\n",
    "#     (817, 818), (819, 820), (821, 822), (823, 824), (825, 826),\n",
    "#     (827, 828), (829, 830), (831, 832), (833, 834), (835, 836),\n",
    "#     (837, 838), (839, 840), (841, 842), (843, 844), (845, 846),\n",
    "#     (847, 848), (849, 850)\n",
    "# ]\n",
    "# unique_images[unique_images_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59bc3b21-e29d-4d2b-8223-cd704e3f058a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/693 [00:00<?, ?it/s]/home/ri4541/.conda/envs/rt_mindEye2/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|██████████| 693/693 [00:11<00:00, 61.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images torch.Size([693, 3, 224, 224])\n",
      "MST_images 693\n",
      "MST_images==True 124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import imageio.v2 as imageio\n",
    "resize_transform = transforms.Resize((224, 224))\n",
    "MST_images = []\n",
    "images = None\n",
    "for im_name in tqdm(image_idx):\n",
    "    if sub == 'sub-001' and session == 'ses-01':\n",
    "        image_file = f\"all_stimuli/rtmindeye_stimuli/{unique_images[im_name]}\"\n",
    "    else:\n",
    "        image_file = f\"{unique_images[im_name]}\"\n",
    "    im = imageio.imread(image_file)\n",
    "    im = torch.Tensor(im / 255).permute(2,0,1)\n",
    "    im = resize_transform(im.unsqueeze(0))\n",
    "    if images is None:\n",
    "        images = im\n",
    "    else:\n",
    "        images = torch.vstack((images, im))\n",
    "    if (sub == 'sub-001' and session == 'ses-04') or (sub == 'sub-003' and session == 'ses-01'):\n",
    "        if ('w_' in image_file or 'paired_image_' in image_file or re.match(r'all_stimuli/rtmindeye_stimuli/\\d{1,2}_\\d{1,3}\\.png$', image_file) or re.match(r'all_stimuli/rtmindeye_stimuli/images/\\d{1,2}_\\d{1,3}\\.png$', image_file)):  \n",
    "            MST_images.append(True)\n",
    "        else:\n",
    "            MST_images.append(False)\n",
    "    else:   \n",
    "        if (\"MST_pairs\" in image_file): # (\"_seed_\" not in unique_images[im_name]) and (unique_images[im_name] != \"blank.jpg\") \n",
    "            MST_images.append(True)\n",
    "        else:\n",
    "            MST_images.append(False)\n",
    "\n",
    "print(\"images\", images.shape)\n",
    "MST_images = np.array(MST_images)\n",
    "print(\"MST_images\", len(MST_images))\n",
    "if (sub == 'sub-001' and session == 'ses-04') or (sub == 'sub-003' and session == 'ses-01'):\n",
    "    assert len(MST_images[MST_images==True]) == 100\n",
    "print(\"MST_images==True\", len(MST_images[MST_images==True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f440a02-dd8a-4a13-9c90-bd07253f6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = utils.find_paired_indices(image_idx)\n",
    "pairs = sorted(pairs, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5f61515-d4fa-419b-b945-cdedc8f24669",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vox (693, 1, 1, 182242)\n",
      "vox (693, 182242)\n"
     ]
    }
   ],
   "source": [
    "vox = None\n",
    "needs_postprocessing = False\n",
    "params = (session, ses_list, remove_close_to_MST, image_names, remove_random_n, vox_idx)\n",
    "\n",
    "if resample_post_glmsingle == True:\n",
    "    glm_save_path_resampled = f\"{glmsingle_path}/vox_resampled.nii.gz\"\n",
    "    if load_from_resampled_file == True:\n",
    "        # resampling was done in this notebook so we can load from file\n",
    "        vox = nib.load(glm_save_path_resampled)\n",
    "    else:\n",
    "        # do resampling here\n",
    "        assert os.path.exists(ref_name) and os.path.exists(omat_name), \"need to generate the boldref and omat separately since we don't have access to the functional data here; either do so using flirt on the command line or copy over the glmsingle resampled outputs\"\n",
    "        vox = load_preprocess_betas(orig_glmsingle_path, *params)\n",
    "        vox = resample_betas(orig_glmsingle_path, sub, session, task_name, vox, glmsingle_path, glm_save_path_resampled, ref_name, omat_name)\n",
    "    needs_postprocessing = True\n",
    "\n",
    "if vox is None:\n",
    "    # either resampling was done in glmsingle or we aren't resampling \n",
    "    vox = load_preprocess_betas(glmsingle_path, *params)\n",
    "\n",
    "if needs_postprocessing == True:\n",
    "    vox = apply_mask(vox, avg_mask)\n",
    "    vox = vox.reshape(-1, vox.shape[-1])  # flatten the 3D image into np array with shape (voxels, images)\n",
    "    print(vox.shape)\n",
    "\n",
    "assert len(vox) == len(image_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4675ba2-b27c-48db-893c-d81f978ba93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/ri4541/MindEyeV2/src/mindeyev2/glmsingle_sub-005_ses-01_task-C/sub-005_ses-01_task-C_brain.nii.gz\n",
      "Mask dimensions: (2.0, 2.0, 2.0)\n",
      "\n",
      "Affine:\n",
      "[[  2.           0.           0.         -76.29234314]\n",
      " [  0.           2.           0.         -84.79180908]\n",
      " [  0.           0.           2.         -62.80359268]\n",
      " [  0.           0.           0.           1.        ]]\n",
      "\n",
      "There are 182242 voxels in the included brain mask\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nilearn.plotting import plot_roi, plot_anat, plot_epi\n",
    "\n",
    "mask_name = f'{glmsingle_path}/{sub}_{session_label}{task_name}_brain'\n",
    "if resample_voxel_size:\n",
    "    if resample_post_glmsingle is True:\n",
    "        # use original mask directory\n",
    "        mask_in_name = f'{orig_glmsingle_path}/{sub}_{session}{task_name}_brain.nii.gz'\n",
    "        mask_out_name = mask_name + f\"_{mask_resampled_suffix}.nii.gz\"\n",
    "        assert os.path.exists(mask_in_name)\n",
    "        applyxfm(mask_in_name, ref_name, omat_name, resample_method, output=mask_out_name)\n",
    "        apply_thresh(mask_out_name, 0.5, output=mask_out_name)  # binarize the mask since resampling can result in non- 0 or 1 values\n",
    "    mask_name += f\"_{mask_resampled_suffix}\"\n",
    "\n",
    "mask_name += \".nii.gz\"\n",
    "print(mask_name)\n",
    "avg_mask = nib.load(mask_name)\n",
    "# mask info\n",
    "dimsize=avg_mask.header.get_zooms()\n",
    "affine_mat = avg_mask.affine\n",
    "brain=avg_mask.get_fdata()\n",
    "xyz=brain.shape #xyz dimensionality of brain mask and epi data\n",
    "\n",
    "print('Mask dimensions:', dimsize)\n",
    "print('')\n",
    "print('Affine:')\n",
    "print(affine_mat)\n",
    "print('')\n",
    "print(f'There are {int(np.sum(brain))} voxels in the included brain mask\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a5573cf-19b5-40e6-b21c-883e762f5f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/ri4541/MindEyeV2/src/mindeyev2/glmsingle_sub-005_ses-01_task-C/sub-005_ses-01_task-C_nsdgeneral.nii.gz\n",
      "nsdgeneral path exists!\n"
     ]
    }
   ],
   "source": [
    "nsdgeneral_path = f'{glmsingle_path}/{sub}_{session_label}{task_name}_nsdgeneral.nii.gz'  \n",
    "print(nsdgeneral_path)\n",
    "assert os.path.exists(nsdgeneral_path)\n",
    "print(f\"nsdgeneral path exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b940e5dc-ac25-4f48-9764-6030cf18ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resample_voxel_size:\n",
    "    nsdgeneral_path = f'{glmsingle_path}/{sub}_task-{task}_nsdgeneral_resampled.nii.gz'  \n",
    "    if resample_post_glmsingle:\n",
    "        assert os.path.exists(orig_glmsingle_path)\n",
    "        roi_in_path = f\"{orig_glmsingle_path}/{sub}_task-{task}_nsdgeneral.nii.gz\"  # the input file is the original nsdgeneral mask (without resampling), from the original glmsingle directory\n",
    "        applyxfm(roi_in_path, ref_name, omat_name, resample_method, output=nsdgeneral_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3187c14-13df-4e51-915c-bb866eec413f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76, 90, 74)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x350 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roi = nib.load(nsdgeneral_path)\n",
    "print(roi.shape)\n",
    "plot_roi(roi, bg_img=avg_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d906312b-ea5d-418d-8326-e8b395c9a9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total voxels (whole brain) = 182242\n",
      "nsdgeneral voxels = 19264\n"
     ]
    }
   ],
   "source": [
    "avg_mask = avg_mask.get_fdata().flatten()\n",
    "print(f\"total voxels (whole brain) = {int(avg_mask.sum())}\")\n",
    "\n",
    "roi = roi.get_fdata()\n",
    "roi = roi.flatten()\n",
    "roi = roi[avg_mask.astype(bool)]\n",
    "roi[np.isnan(roi)] = 0\n",
    "roi = roi.astype(bool)\n",
    "print(f\"nsdgeneral voxels = {roi.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce12274a-3b35-444d-92b0-7cfd0949badc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vox before ROI exclusion: (693, 182242)\n",
      "vox after ROI exclusion: (693, 19264)\n"
     ]
    }
   ],
   "source": [
    "# ROI masking?\n",
    "print(f\"vox before ROI exclusion: {vox.shape}\")\n",
    "vox = vox[:,roi]\n",
    "print(f\"vox after ROI exclusion: {vox.shape}\")\n",
    "\n",
    "if np.any(np.isnan(vox)):\n",
    "    print(\"NaNs found! Removing voxels...\")\n",
    "    x,y = np.where(np.isnan(vox))\n",
    "    vox = vox[:,np.setdiff1d(np.arange(vox.shape[-1]), y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26802a5b-7bc8-4d47-b8e0-1dfa557fc6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_homog = np.array([[p[0], p[1]] for p in pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50d52f93-af1d-448d-92e4-5af8096aaaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19264/19264 [00:01<00:00, 18091.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rels (19264,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vox_pairs = utils.zscore(vox[pairs_homog])\n",
    "rels = np.full(vox.shape[-1],np.nan)\n",
    "for v in tqdm(range(vox.shape[-1])):\n",
    "    rels[v] = np.corrcoef(vox_pairs[:,0,v], vox_pairs[:,1,v])[1,0]\n",
    "print(\"rels\", rels.shape)\n",
    "assert np.sum(np.all(np.isnan(rels))) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84be077b-fbef-4b23-895c-4928228229d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 19264, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:00<00:00, 6020.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating img x vox x repetitions matrix | shape=(150, 18419, 2)\n",
    "vox0 = np.zeros((len(pairs_homog), vox.shape[-1], 2))\n",
    "print(vox0.shape)\n",
    "for ipair, pair in enumerate(tqdm(pairs_homog)):\n",
    "    pair = pair[:2] # to keep things consistent, just using the first two repeats\n",
    "    i,j = pair\n",
    "    vox0[ipair, :, :] = vox[pair].T\n",
    "vox_avg = vox0.mean(-1) # average across the repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6206a31e-3d0a-4a30-ada2-4cffa1009856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vox before reliability thresholding: (693, 19264)\n",
      "\n",
      "vox after reliability thresholding: (693, 2794)\n"
     ]
    }
   ],
   "source": [
    "# Reliability thresholding?\n",
    "print(f\"\\nvox before reliability thresholding: {vox.shape}\")\n",
    "vox = vox[:,rels>.2]\n",
    "print(f\"\\nvox after reliability thresholding: {vox.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6f632cc-2b26-4dc8-a4d5-12b641765601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([693, 3, 224, 224])\n",
      "(693, 2794)\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)\n",
    "print(vox.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "735dfc27-a9bd-4a22-ac3f-a1f44515293e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 124\n"
     ]
    }
   ],
   "source": [
    "utils.seed_everything(seed)\n",
    "\n",
    "# add_repeats = 48\n",
    "# imageTrain = np.arange(len(images))\n",
    "# train_image_indices = np.array([item for item in imageTrain if item not in pairs.flatten()])\n",
    "# train_image_indices = np.sort(np.append(train_image_indices, np.array(pairs[:add_repeats].flatten())))\n",
    "\n",
    "# # check that there's no repeat indices in training data\n",
    "# assert len(sorted(np.append(np.array([item for item in imageTrain if item not in pairs.flatten()]), np.array(pairs[:add_repeats].flatten())))) == len(set(sorted(np.append(np.array([item for item in imageTrain if item not in pairs.flatten()]), np.array(pairs[:add_repeats].flatten())))))\n",
    "\n",
    "# test_image_indices = pairs[add_repeats:]\n",
    "# print(len(train_image_indices), len(test_image_indices))\n",
    "\n",
    "if train_test_split == 'orig':\n",
    "    # train = all images except images that were repeated\n",
    "    # test = average of the same-image presentations\n",
    "    imageTrain = np.arange(len(images))\n",
    "    train_image_indices = np.array([item for item in imageTrain if item not in pairs.flatten()])\n",
    "    test_image_indices = pairs\n",
    "    print(len(train_image_indices), len(test_image_indices))\n",
    "elif train_test_split == 'MST':\n",
    "    # non-MST images are the train split\n",
    "    # MST images are the test split\n",
    "    train_image_indices = np.where(MST_images==False)[0]\n",
    "    test_image_indices = np.where(MST_images==True)[0]\n",
    "    print(len(train_image_indices), len(test_image_indices))\n",
    "    # for i in test_image_indices:\n",
    "    #     assert i in pairs  # all MST images have pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a292cfad-83f4-4bf8-994e-da2c871c0a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_image_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b81220cd-c11d-4a2a-8755-53b70d90cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeats_in_test = []\n",
    "# for p in pairs:\n",
    "#     group = []\n",
    "#     for item in p:\n",
    "#         curr = np.where(test_image_indices == item)\n",
    "#         if curr[0].size > 0:\n",
    "#             group.append(curr[0][0])\n",
    "#     # print(np.array(group))\n",
    "#     if len(group) > 0:\n",
    "#         repeats_in_test.append(np.array(group))\n",
    "#     # if p[0] in test_image_indices:\n",
    "#     #     repeats_in_test.append(p)\n",
    "        \n",
    "# repeats_in_test = np.array(repeats_in_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5528d877-b662-41f7-8982-3f31051871f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voxels have been zscored\n",
      "-0.11855685 1.003355\n",
      "vox (693, 2794)\n"
     ]
    }
   ],
   "source": [
    "train_mean = np.mean(vox[train_image_indices],axis=0)\n",
    "train_std = np.std(vox[train_image_indices],axis=0)\n",
    "\n",
    "vox = utils.zscore(vox,train_mean=train_mean,train_std=train_std)\n",
    "print(\"voxels have been zscored\")\n",
    "print(vox[:,0].mean(), vox[:,0].std())\n",
    "print(\"vox\", vox.shape)\n",
    "\n",
    "images = torch.Tensor(images)\n",
    "vox = torch.Tensor(vox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1eb5d464-7ffa-419a-a6b4-d0108f8e196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.utils.data.TensorDataset(torch.tensor(test_image_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3901c-60dd-4ae2-b0f5-8a55aa231908",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64672583-9f00-46f5-8d4e-00e4c7068a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test dl for subj1!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subj_list = [subj]\n",
    "subj = subj_list[0]\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=len(test_data), shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3cbeea8-e95b-48d9-9bc2-91af260c93d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 124 124\n"
     ]
    }
   ],
   "source": [
    "test_voxels, test_images = None, None\n",
    "for test_i, behav in enumerate(test_dl):\n",
    "    behav = behav[0]\n",
    "\n",
    "    if behav.ndim>1:\n",
    "        test_image = images[behav[:,0].long().cpu()].to(device)\n",
    "        test_vox = vox[behav.long().cpu()].mean(1)\n",
    "    else:\n",
    "        test_image = images[behav.long().cpu()].to(device)\n",
    "        test_vox = vox[behav.long().cpu()]\n",
    "    \n",
    "    if test_voxels is None:\n",
    "        test_voxels = test_vox\n",
    "        test_images = test_image\n",
    "    else:\n",
    "        test_voxels = torch.vstack((test_voxels, test_vox))\n",
    "        test_images = torch.vstack((test_images, test_image))\n",
    "\n",
    "print(test_i, len(test_voxels), len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3ae7a06-7135-4073-b315-59579e35e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_voxels_list = []\n",
    "num_voxels_list.append(test_voxels.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de0400d4-cbd6-4941-a0b2-1a4bc2ae97da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## USING OpenCLIP ViT-bigG ###\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder\n",
    "\n",
    "try:\n",
    "    print(clip_img_embedder)\n",
    "except:\n",
    "    clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "        arch=\"ViT-bigG-14\",\n",
    "        version=\"laion2b_s39b_b160k\",\n",
    "        output_tokens=True,\n",
    "        only_tokens=True,\n",
    "    )\n",
    "    clip_img_embedder.to(device)\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6e720891-6575-4711-bc35-876527ed3b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---loading /scratch/gpfs/ri4541/MindEyeV2/src/mindeyev2/train_logs/sub-005_ses-01_task-C_bs24_MST_rishab_MSTsplit/last.pth ckpt---\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MindEyeModule:\n\tsize mismatch for ridge.linears.0.weight: copying a param with shape torch.Size([1024, 2798]) from checkpoint, the shape in current model is torch.Size([1024, 2794]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(outdir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m checkpoint\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/rt_mindEye2/lib/python3.11/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MindEyeModule:\n\tsize mismatch for ridge.linears.0.weight: copying a param with shape torch.Size([1024, 2798]) from checkpoint, the shape in current model is torch.Size([1024, 2794])."
     ]
    }
   ],
   "source": [
    "# Load pretrained model ckpt\n",
    "tag='last'\n",
    "outdir = os.path.abspath(f'/scratch/gpfs/ri4541/MindEyeV2/src/mindeyev2/train_logs/{model_name}')\n",
    "print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "checkpoint = torch.load(outdir+f'/{tag}.pth', map_location='cpu')\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "del checkpoint\n",
    "print(\"ckpt loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "56b606a4-7302-4ac5-b89d-bbe4fcb00d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e452b5b2-47d9-4271-b9fc-ea331fbac1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MindEyeModule()\n",
      "param counts:\n",
      "2,862,080 total\n",
      "2,862,080 trainable\n",
      "param counts:\n",
      "2,862,080 total\n",
      "2,862,080 trainable\n",
      "param counts:\n",
      "453,360,280 total\n",
      "453,360,280 trainable\n",
      "param counts:\n",
      "456,222,360 total\n",
      "456,222,360 trainable\n",
      "param counts:\n",
      "259,865,216 total\n",
      "259,865,200 trainable\n",
      "param counts:\n",
      "716,087,576 total\n",
      "716,087,560 trainable\n"
     ]
    }
   ],
   "source": [
    "model = utils.prepare_model_and_training(\n",
    "    num_voxels_list=num_voxels_list,\n",
    "    n_blocks=n_blocks,\n",
    "    hidden_dim=hidden_dim,\n",
    "    clip_emb_dim=clip_emb_dim,\n",
    "    clip_seq_dim=clip_seq_dim,\n",
    "    use_prior=use_prior,\n",
    "    clip_scale=clip_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f726f617-39f5-49e2-8d0c-d11d27d01c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n"
     ]
    }
   ],
   "source": [
    "# prep unCLIP\n",
    "config = OmegaConf.load(\"/scratch/gpfs/ri4541/MindEyeV2/src/generative_models/configs/unclip6.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "unclip_params = config[\"model\"][\"params\"]\n",
    "network_config = unclip_params[\"network_config\"]\n",
    "denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "sampler_config = unclip_params[\"sampler_config\"]\n",
    "scale_factor = unclip_params[\"scale_factor\"]\n",
    "disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "diffusion_engine = DiffusionEngine(network_config=network_config,\n",
    "                       denoiser_config=denoiser_config,\n",
    "                       first_stage_config=first_stage_config,\n",
    "                       conditioner_config=conditioner_config,\n",
    "                       sampler_config=sampler_config,\n",
    "                       scale_factor=scale_factor,\n",
    "                       disable_first_stage_autocast=disable_first_stage_autocast)\n",
    "# set to inference\n",
    "diffusion_engine.eval().requires_grad_(False)\n",
    "diffusion_engine.to(device)\n",
    "\n",
    "ckpt_path = '/scratch/gpfs/ri4541/MindEyeV2/src/mindeyev2/unclip6_epoch0_step110000.ckpt' \n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "batch={\"jpg\": torch.randn(1,3,1,1).to(device), # jpg doesnt get used, it's just a placeholder\n",
    "      \"original_size_as_tuple\": torch.ones(1, 2).to(device) * 768,\n",
    "      \"crop_coords_top_left\": torch.zeros(1, 2).to(device)}\n",
    "out = diffusion_engine.conditioner(batch)\n",
    "vector_suffix = out[\"vector\"].to(device)\n",
    "print(\"vector_suffix\", vector_suffix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68abd440-7e6b-4023-9dc8-05b1b5c0baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup text caption networks\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from modeling_git import GitForCausalLMClipEmb\n",
    "# processor = AutoProcessor.from_pretrained(\"microsoft/git-large-coco\")\n",
    "# clip_text_model = GitForCausalLMClipEmb.from_pretrained(\"microsoft/git-large-coco\")\n",
    "processor = AutoProcessor.from_pretrained(\"/scratch/gpfs/ri4541/MindEyeV2/src/mindeyev2\")\n",
    "clip_text_model = GitForCausalLMClipEmb.from_pretrained(\"/scratch/gpfs/ri4541/MindEyeV2/src/mindeyev2\")\n",
    "\n",
    "clip_text_model.to(device) # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "clip_text_model.eval().requires_grad_(False)\n",
    "clip_text_seq_dim = 257\n",
    "clip_text_emb_dim = 1024\n",
    "\n",
    "class CLIPConverter(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIPConverter, self).__init__()\n",
    "        self.linear1 = nn.Linear(clip_seq_dim, clip_text_seq_dim)\n",
    "        self.linear2 = nn.Linear(clip_emb_dim, clip_text_emb_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x.permute(0,2,1))\n",
    "        return x\n",
    "        \n",
    "clip_convert = CLIPConverter()\n",
    "state_dict = torch.load(\"/scratch/gpfs/ri4541/MindEyeV2/src/mindeyev2/bigG_to_L_epoch8.pth\", map_location='cpu')['model_state_dict']\n",
    "clip_convert.load_state_dict(state_dict, strict=True)\n",
    "clip_convert.to(device) # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "del state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6a706a3-d151-4643-bb34-7d08aa7361c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1899d669587f464ba356a29615d5b8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a kitchen with a counter and a microwave.', 'a cat is sitting on a table.', 'a large room with a lot of furniture.', 'a giraffe standing next to a tree.', 'a room with a view.', 'a room with a lot of furniture.', 'a large field with a lot of grass.', 'a kitchen with a lot of furniture.', 'a large room with a lot of furniture.', 'a garden with a plant and a fence.', 'a kitchen with a lot of furniture.', 'a kitchen with a counter and a stove', 'a room with a view', 'a plate with a cake on it', 'a snowboarder is skiing down a hill.', 'a clock on a building.', 'a young boy is standing in a pool of water.', 'a bed or beds in a room at the inn', 'a large building with a clock on it.', 'a room with a lot of furniture.', 'a table with a bunch of items on it', 'a tree with a lot of leaves.', 'a night view of a city.', 'a white wall', 'a large building with a clock on it.', 'a large group of people.', 'a small room with a clock and a vase.', 'a large truck is parked next to a building.', 'a room with a view.', 'a table with a lot of items on it', 'a picture of a room with a lot of things in it.', 'a room with a lot of furniture.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ri4541/.conda/envs/rt_mindEye2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ri4541/.conda/envs/rt_mindEye2/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      " 25%|██▌       | 1/4 [02:21<07:04, 141.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-001_ses-01_bs24_MST_rishab_MSTsplit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0628499d46941cf9f65527d8eb5d525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a train is driving through a field.', 'a cat sitting on a table.', 'a zebra standing in a field.', 'a plate of food', 'a car driving down a street.', 'a large field with a bunch of people on it', 'a man on a boat in a lake.', 'a view of a table.', 'a large area with a lot of grass.', 'a display of a cell phone.', 'a bunch of different types of flowers', 'a large open area with a lot of space for a small table.', 'a room with a lot of furniture.', 'a large planter with a bunch of flowers on it.', 'a bathroom with a shower and a sink.', 'a large body of water.', 'a street light and a street sign', 'a plate with a piece of food on it', 'a room with a view', 'a bunch of flowers on a table.', 'a small table with a small display.', 'a glass door with a window.', 'a stuffed toy bear is sitting on a table.', 'a small tree in a field.', 'a plate of food with a fork.', 'a snowboarder is on a hill.', 'a room with a lot of furniture.', 'a bathroom with a toilet and a sink.', 'a table with a bunch of food on it', 'a small white and black wall', 'a person sitting down.', 'a clock tower with a clock on it.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [04:42<04:42, 141.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-001_ses-01_bs24_MST_rishab_MSTsplit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518fa75876214f5f857ebdb9c0c9da3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a man on a surfboard in the water.', 'a woman standing on a sidewalk next to a water.', 'a table with a plate and a plate on it', 'a large truck is parked on the side of the road.', 'a clock tower with a tower in the background.', 'a room with a lot of furniture.', 'a white wall with a window', 'a glass door with a window.', 'a picture of a tree.', 'a view of a large room.', 'a grassy field with a few animals in it.', 'a white wall', 'a plate of food with a fork.', 'a plate with a piece of food on it', 'a clock tower with a tower in the background.', 'a clock tower with a tower in the background.', 'a room with a view.', 'a laptop computer sitting on top of a table.', 'a zebra standing on a dirt field.', 'a surfer riding a wave on a sunny day.', 'a toilet with a lid', 'a room with a table and chairs.', 'a kitchen with a sink and a counter', 'a bathroom with a sink and a mirror.', \"a close up of a person's head\", 'a group of animals standing on a field.', 'a white table with a glass top', 'a white room with a toilet and a sink', 'a plate of food with a fork on it.', 'a picture of a large group of trees.', 'a table with a plate of food on it', 'a kitchen with a counter and a sink.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [07:04<02:21, 141.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-001_ses-01_bs24_MST_rishab_MSTsplit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c339474faf47bc9795754130e0060b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a room with a bed and a television.', 'a large building with a clock on it.', 'a man on a surfboard in the water.', 'a train is driving down the tracks.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [07:22<00:00, 110.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-001_ses-01_bs24_MST_rishab_MSTsplit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/ri4541/.conda/envs/rt_mindEye2/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved sub-001_ses-01_bs24_MST_rishab_MSTsplit outputs!\n"
     ]
    }
   ],
   "source": [
    "# get all reconstructions\n",
    "model.to(device)\n",
    "model.eval().requires_grad_(False)\n",
    "\n",
    "all_blurryrecons = None\n",
    "all_images = None\n",
    "all_recons = None\n",
    "all_predcaptions = []\n",
    "all_clipvoxels = None\n",
    "all_prior_out = None\n",
    "all_backbones = None\n",
    "\n",
    "minibatch_size = 32\n",
    "num_samples_per_image = 1\n",
    "plotting = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(range(0,len(test_images),minibatch_size)):\n",
    "        start_time = time.time() \n",
    "\n",
    "        image = test_images[batch:batch+minibatch_size]\n",
    "        voxel = test_voxels[batch:batch+minibatch_size].unsqueeze(1).to(device)\n",
    "\n",
    "        # Save ground truth images\n",
    "        if all_images is None:\n",
    "            all_images = image\n",
    "        else:\n",
    "            all_images = torch.vstack((all_images, image))\n",
    "        \n",
    "        voxel_ridge = model.ridge(voxel,0)\n",
    "        backbone, clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge)\n",
    "                \n",
    "        # Save retrieval submodule outputs\n",
    "        if clip_scale>0:\n",
    "            if all_clipvoxels is None:\n",
    "                all_clipvoxels = clip_voxels.cpu()\n",
    "            else:\n",
    "                all_clipvoxels = torch.vstack((all_clipvoxels, clip_voxels.cpu()))\n",
    "                \n",
    "        # Feed voxels through OpenCLIP-bigG diffusion prior\n",
    "        prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, \n",
    "                        text_cond = dict(text_embed = backbone), \n",
    "                        cond_scale = 1., timesteps = 20).cpu()\n",
    "        \n",
    "        if all_prior_out is None:\n",
    "            all_prior_out = prior_out\n",
    "        else:\n",
    "            all_prior_out = torch.vstack((all_prior_out, prior_out))\n",
    "\n",
    "        pred_caption_emb = clip_convert(prior_out.to(device).float())\n",
    "        generated_ids = clip_text_model.generate(pixel_values=pred_caption_emb, max_length=20)\n",
    "        generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_predcaptions = np.hstack((all_predcaptions, generated_caption))\n",
    "        print(generated_caption)\n",
    "        \n",
    "        # Feed diffusion prior outputs through unCLIP\n",
    "        if plotting:\n",
    "            jj=-1\n",
    "            fig, axes = plt.subplots(1, 12, figsize=(10, 4))\n",
    "\n",
    "        for i in range(len(voxel)):\n",
    "            samples = utils.unclip_recon(prior_out[[i]],\n",
    "                             diffusion_engine,\n",
    "                             vector_suffix,\n",
    "                             num_samples=num_samples_per_image)\n",
    "            if all_recons is None:\n",
    "                all_recons = samples.cpu()\n",
    "            else:\n",
    "                all_recons = torch.vstack((all_recons, samples.cpu()))\n",
    "                \n",
    "            if plotting:  \n",
    "                jj+=1\n",
    "                axes[jj].imshow(utils.torch_to_Image(image[i]))\n",
    "                axes[jj].axis('off')\n",
    "                jj+=1\n",
    "                axes[jj].imshow(utils.torch_to_Image(samples.cpu()[0]))\n",
    "                axes[jj].axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        print(model_name)\n",
    "        # err # dont actually want to run the whole thing with plotting=True\n",
    "\n",
    "# resize outputs before saving\n",
    "imsize = 256\n",
    "all_images = transforms.Resize((imsize,imsize))(all_images).float()\n",
    "all_recons = transforms.Resize((imsize,imsize))(all_recons).float()\n",
    "if blurry_recon: \n",
    "    all_blurryrecons = transforms.Resize((imsize,imsize))(all_blurryrecons).float()\n",
    "        \n",
    "## Saving ##\n",
    "if not os.path.exists(eval_dir):\n",
    "    os.mkdir(eval_dir)\n",
    "\n",
    "if \"MST\" in model_name:\n",
    "    np.save(f\"{eval_dir}/{model_name}_MST_ID.npy\", MST_ID)\n",
    "torch.save(all_images.cpu(),f\"{eval_dir}/{model_name}_all_images.pt\")\n",
    "\n",
    "# repeats_in_test = []\n",
    "# for p in pairs:\n",
    "#     if p[0] in test_image_indices:\n",
    "#         repeats_in_test.append(p)\n",
    "        \n",
    "# repeats_in_test = np.array(repeats_in_test)\n",
    "\n",
    "# torch.save(test_image_indices, f\"{eval_dir}/{model_name}_test_image_indices.pt\")\n",
    "# torch.save(repeats_in_test, f\"{eval_dir}/{model_name}_repeats_in_test.pt\")\n",
    "torch.save(all_recons,f\"{eval_dir}/{model_name}_all_recons.pt\")\n",
    "if clip_scale>0:\n",
    "    torch.save(all_clipvoxels,f\"{eval_dir}/{model_name}_all_clipvoxels.pt\")\n",
    "torch.save(all_prior_out,f\"{eval_dir}/{model_name}_all_prior_out.pt\")\n",
    "torch.save(all_predcaptions,f\"{eval_dir}/{model_name}_all_predcaptions.pt\")\n",
    "print(f\"saved {model_name} outputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b243d7-6552-4fc8-bef7-d5ad03b17cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"MST\" in model_name:\n",
    "    np.save(f\"{eval_dir}/{model_name}_MST_ID.npy\", MST_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6856c3-9205-48f5-bfb2-7e0099f429a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a7162f-ca3b-4b14-9676-3037094994c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.permute(all_images, (0,2,3,1))\n",
    "y = torch.permute(all_recons, (0,2,3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa41429-ab6a-4aa6-96b9-5c963016b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 2, figsize=(8, 8))\n",
    "for row, _ in enumerate(ax):\n",
    "    ax[row][0].imshow(x.cpu()[row])\n",
    "    ax[row][1].imshow(y.cpu()[row])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d553a7b3-9bdf-44b3-a0bf-398cf5cf402b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rt_mindEye2 [~/.conda/envs/rt_mindEye2/]",
   "language": "python",
   "name": "conda_rt_mindeye2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
