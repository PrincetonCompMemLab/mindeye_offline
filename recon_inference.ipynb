{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16c9d4c-66cb-4692-a61d-9aa86a8765d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder, FrozenCLIPEmbedder, FrozenOpenCLIPEmbedder2\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from generative_models.sgm.util import append_dims\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "from models import *\n",
    "\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "accelerator = Accelerator(split_batches=False)\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d8de1-d0ca-4b5f-84d8-2560f0399a5a",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c47b5b-869f-468c-bb93-43610ee5dbe0",
   "metadata": {},
   "source": [
    "## New Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c1e0c6-0641-4239-8201-f2c676532302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/weka/proj-fmri/paulscotti/MindEyeV2/src/rtmindeye/csv/sub-001.csv\n",
      "len_unique_images 851\n",
      "n_runs 16\n",
      "['images/image_686_seed_1.png' 'images/image_262_seed_1.png'\n",
      " 'images/image_508_seed_1.png' 'images/image_671_seed_1.png']\n",
      "[742.49907775 746.4931488  750.48907224 754.489129  ]\n",
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "sub = \"sub-001\"\n",
    "session = \"ses-02\"\n",
    "n_runs = 16\n",
    "\n",
    "filename = f\"csv/{sub}_{session}.csv\"\n",
    "print(filename)\n",
    "data = pd.read_csv(filename)\n",
    "images = data['current_image'].values[14:] # note: used to be [23:] in the earlier scans\n",
    "starts = data['trial.started'].values[14:]\n",
    "is_new_run = data['is_new_run'].values[14:]\n",
    "\n",
    "unique_images = np.unique(images.astype(str))\n",
    "unique_images = unique_images[(unique_images!=\"nan\")]\n",
    "unique_images = unique_images[(unique_images!=\"blank.jpg\")]\n",
    "len_unique_images = len(unique_images)\n",
    "print(\"len_unique_images\",len_unique_images)\n",
    "print(\"n_runs\",n_runs)\n",
    "\n",
    "print(images[:4])\n",
    "print(starts[:4])\n",
    "print(is_new_run[:4])\n",
    "\n",
    "image_idx = np.array([])\n",
    "for i in range(len(images)):\n",
    "    if images[i] == \"blank.jpg\":\n",
    "        continue\n",
    "    if str(images[i]) == \"nan\":\n",
    "        continue\n",
    "\n",
    "    image_idx_ = np.where(images[i]==unique_images)[0].item()\n",
    "    image_idx = np.append(image_idx, image_idx_)\n",
    "image_idx = torch.Tensor(image_idx).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b4ecca-0892-4bbd-b4b3-227247b3b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_images_pairs = [\n",
    "    (0, 1), (2, 3), (4, 5), (6, 7), (8, 9), (10, 11),\n",
    "    (12, 13), (14, 15), (16, 17), (18, 19), (20, 21),\n",
    "    (22, 23), (24, 25), (26, 27), (28, 29), (30, 31),\n",
    "    (32, 33), (34, 35), (36, 37), (38, 39), (40, 41),\n",
    "    (42, 43), (44, 45), (46, 47), (48, 49),\n",
    "]\n",
    "print(unique_images[unique_images_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ceb404f-b04f-42b6-afc4-283bb2b40c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_737884/3490374544.py:7: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  im = imageio.imread(f\"rtmindeye_stimuli/{(unique_images[im_name]).split('/')[-1]}\")\n",
      "/admin/home-paulscotti/mindeye/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████| 1000/1000 [00:50<00:00, 19.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images torch.Size([1000, 3, 224, 224])\n",
      "MST_images 1000\n",
      "MST_images==True 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "resize_transform = transforms.Resize((224, 224))\n",
    "MST_images = []\n",
    "images = None\n",
    "for im_name in tqdm(image_idx):\n",
    "    # im = imageio.imread(f\"rtmindeye_stimuli/{(unique_images[im_name]).split('/')[-1]}\")\n",
    "    im = imageio.imread(f\"{unique_images[im_name]}\")\n",
    "    im = torch.Tensor(im / 255).permute(2,0,1)\n",
    "    im = resize_transform(im.unsqueeze(0))\n",
    "    if images is None:\n",
    "        images = im\n",
    "    else:\n",
    "        images = torch.vstack((images, im))\n",
    "    if (\"MST_pairs\" in unique_images[im_name]): # (\"_seed_\" not in unique_images[im_name]) and (unique_images[im_name] != \"blank.jpg\") \n",
    "        MST_images.append(True)\n",
    "    else:\n",
    "        MST_images.append(False)\n",
    "\n",
    "print(\"images\", images.shape)\n",
    "MST_images = np.array(MST_images)\n",
    "print(\"MST_images\", len(MST_images))\n",
    "print(\"MST_images==True\", len(MST_images[MST_images==True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f440a02-dd8a-4a13-9c90-bd07253f6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = utils.find_paired_indices(image_idx)\n",
    "pairs = np.array(sorted(pairs, key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5f61515-d4fa-419b-b945-cdedc8f24669",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vox (1000, 165240)\n"
     ]
    }
   ],
   "source": [
    "glmsingle = np.load(f\"glmsingle_{session}/TYPED_FITHRF_GLMDENOISE_RR.npz\",allow_pickle=True) \n",
    "\n",
    "vox = glmsingle['betasmd'].T\n",
    "print(\"vox\", vox.shape)\n",
    "\n",
    "if vox.ndim==4:\n",
    "    vox = vox[:,0,0]\n",
    "    print(\"vox\", vox.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a5573cf-19b5-40e6-b21c-883e762f5f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask dimensions: (2.0, 2.0, 2.0)\n",
      "\n",
      "Affine:\n",
      "[[   2.     0.     0.   -73.5]\n",
      " [   0.     2.     0.  -109. ]\n",
      " [   0.     0.     2.   -57. ]\n",
      " [   0.     0.     0.     1. ]]\n",
      "\n",
      "There are 165240.0 voxels in the included brain mask\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<nilearn.plotting.displays._slicers.OrthoSlicer at 0x7f08a14f94d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x350 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nilearn.plotting import plot_roi, plot_anat, plot_epi\n",
    "\n",
    "# avg_mask=nib.load(f'{sub}_ses-01_brain.nii.gz')\n",
    "avg_mask=nib.load(f'masks/{sub}_{session}_brain.nii.gz')\n",
    "\n",
    "# mask info\n",
    "dimsize=avg_mask.header.get_zooms()\n",
    "affine_mat = avg_mask.affine\n",
    "brain=avg_mask.get_fdata()\n",
    "xyz=brain.shape #xyz dimensionality of brain mask and epi data\n",
    "\n",
    "print('Mask dimensions:', dimsize)\n",
    "print('')\n",
    "print('Affine:')\n",
    "print(affine_mat)\n",
    "print('')\n",
    "print(f'There are {np.sum(brain)} voxels in the included brain mask\\n')\n",
    "\n",
    "roi = nib.load(f'masks/{sub}_nsdgeneral.nii.gz')\n",
    "\n",
    "plot_roi(roi, bg_img=avg_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d906312b-ea5d-418d-8326-e8b395c9a9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total voxels (whole brain) = 165240\n",
      "nsdgeneral voxels = 18419\n"
     ]
    }
   ],
   "source": [
    "avg_mask = avg_mask.get_fdata().flatten()\n",
    "print(f\"total voxels (whole brain) = {int(avg_mask.sum())}\")\n",
    "\n",
    "roi = roi.get_fdata()\n",
    "roi = roi.flatten()\n",
    "roi = roi[avg_mask.astype(bool)]\n",
    "roi[np.isnan(roi)] = 0\n",
    "roi = roi.astype(bool)\n",
    "print(f\"nsdgeneral voxels = {roi.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce12274a-3b35-444d-92b0-7cfd0949badc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vox after ROI exclusion: (1000, 18419)\n"
     ]
    }
   ],
   "source": [
    "# ROI masking?\n",
    "print(f\"vox before ROI exclusion: {vox.shape}\")\n",
    "vox = vox = vox[:,roi]\n",
    "print(f\"vox after ROI exclusion: {vox.shape}\")\n",
    "\n",
    "if np.any(np.isnan(vox)):\n",
    "    print(\"NaNs found! Removing voxels...\")\n",
    "    x,y = np.where(np.isnan(vox))\n",
    "    vox = vox[:,np.setdiff1d(np.arange(vox.shape[-1]), y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84be077b-fbef-4b23-895c-4928228229d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 18419/18419 [00:00<00:00, 19317.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rels (18419,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vox_pairs = vox[pairs]\n",
    "rels = np.full(vox.shape[-1],np.nan)\n",
    "for v in tqdm(range(vox.shape[-1])):\n",
    "    rels[v] = np.corrcoef(vox_pairs[:,0,v], vox_pairs[:,1,v])[1,0]\n",
    "print(\"rels\", rels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6206a31e-3d0a-4a30-ada2-4cffa1009856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reliability thresholding?\n",
    "print(f\"\\nvox before reliability thresholding: {vox.shape}\")\n",
    "vox = vox[:,rels>.2]\n",
    "print(f\"\\nvox after reliability thresholding: {vox.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6f632cc-2b26-4dc8-a4d5-12b641765601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 3, 224, 224])\n",
      "(1000, 3428)\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)\n",
    "print(vox.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "735dfc27-a9bd-4a22-ac3f-a1f44515293e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 150\n"
     ]
    }
   ],
   "source": [
    "utils.seed_everything(0)\n",
    "\n",
    "# # train = all images except images that were repeated\n",
    "# # test = average of the same-image presentations\n",
    "# imageTrain = np.arange(len(images))\n",
    "# train_image_indices = np.array([item for item in imageTrain if item not in pairs.flatten()])\n",
    "# test_image_indices = pairs\n",
    "# print(len(train_image_indices), len(test_image_indices))\n",
    "\n",
    "# non-MST images are the train split\n",
    "# MST images are the test split\n",
    "train_image_indices = np.where(MST_images==False)[0]\n",
    "test_image_indices = np.where(MST_images==True)[0]\n",
    "print(len(train_image_indices), len(test_image_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5528d877-b662-41f7-8982-3f31051871f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voxels have been zscored\n",
      "-0.020418098 0.9553846\n",
      "vox (1000, 3428)\n"
     ]
    }
   ],
   "source": [
    "train_mean = np.mean(vox[train_image_indices],axis=0)\n",
    "train_std = np.std(vox[train_image_indices],axis=0)\n",
    "\n",
    "vox = utils.zscore(vox,train_mean=train_mean,train_std=train_std)\n",
    "print(\"voxels have been zscored\")\n",
    "print(vox[:,0].mean(), vox[:,0].std())\n",
    "print(\"vox\", vox.shape)\n",
    "\n",
    "images = torch.Tensor(images)\n",
    "vox = torch.Tensor(vox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eb5d464-7ffa-419a-a6b4-d0108f8e196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.utils.data.TensorDataset(torch.tensor(test_image_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3901c-60dd-4ae2-b0f5-8a55aa231908",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e52985b1-95ff-487b-8b2d-cc1ad1c190b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: sub-001_bs24_4gpu\n",
      "--data_path=/weka/proj-fmri/shared/mindeyev2_dataset                     --model_name=sub-001_bs24_4gpu --subj=1                     --no-blurry_recon --use_prior                     --hidden_dim=1024 --n_blocks=4\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"sub-001_bs24_4gpu\" #\"sub-001_bs24_MST\" #\"sub-001_bs24_4gpu\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # global_batch_size and batch_size should already be defined in the above cells\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=/weka/proj-fmri/shared/mindeyev2_dataset \\\n",
    "                    --model_name={model_name} --subj=1 \\\n",
    "                    --no-blurry_recon --use_prior \\\n",
    "                    --hidden_dim=1024 --n_blocks=4\"\n",
    "    \n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49e5dae4-606d-4dc6-b420-df9e4c14737e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"will load ckpt for model found in ../train_logs/model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/weka/proj-fmri/shared/mindeyev2_dataset\",\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=2048,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_len\",type=int,default=1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(\"evals\",exist_ok=True)\n",
    "os.makedirs(f\"evals/{model_name}\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64672583-9f00-46f5-8d4e-00e4c7068a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test dl for subj1!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subj_list = [subj]\n",
    "subj = subj_list[0]\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=len(test_data), shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3cbeea8-e95b-48d9-9bc2-91af260c93d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 150 150\n"
     ]
    }
   ],
   "source": [
    "test_voxels, test_images = None, None\n",
    "for test_i, behav in enumerate(test_dl):\n",
    "    behav = behav[0]\n",
    "\n",
    "    if behav.ndim==2:\n",
    "        test_image = images[behav[:,0].long().cpu()].to(device)\n",
    "        test_vox = vox[behav.long().cpu()].mean(1)\n",
    "    else:\n",
    "        test_image = images[behav.long().cpu()].to(device)\n",
    "        test_vox = vox[behav.long().cpu()]\n",
    "    \n",
    "    if test_voxels is None:\n",
    "        test_voxels = test_vox\n",
    "        test_images = test_image\n",
    "    else:\n",
    "        test_voxels = torch.vstack((test_voxels, test_vox))\n",
    "        test_images = torch.vstack((test_images, test_image))\n",
    "\n",
    "print(test_i, len(test_voxels), len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3ae7a06-7135-4073-b315-59579e35e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_voxels_list = []\n",
    "num_voxels_list.append(test_voxels.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de0400d4-cbd6-4941-a0b2-1a4bc2ae97da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## USING OpenCLIP ViT-bigG ###\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder\n",
    "\n",
    "try:\n",
    "    print(clip_img_embedder)\n",
    "except:\n",
    "    clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "        arch=\"ViT-bigG-14\",\n",
    "        version=\"laion2b_s39b_b160k\",\n",
    "        output_tokens=True,\n",
    "        only_tokens=True,\n",
    "    )\n",
    "    clip_img_embedder.to(device)\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0dcf6f1-a67b-4b39-90be-4b4a4a4f6716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "3,511,296 total\n",
      "3,511,296 trainable\n",
      "param counts:\n",
      "3,511,296 total\n",
      "3,511,296 trainable\n",
      "param counts:\n",
      "453,360,280 total\n",
      "453,360,280 trainable\n",
      "param counts:\n",
      "456,871,576 total\n",
      "456,871,576 trainable\n",
      "param counts:\n",
      "259,865,216 total\n",
      "259,865,200 trainable\n",
      "param counts:\n",
      "716,736,792 total\n",
      "716,736,776 trainable\n"
     ]
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "\n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len=1): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx=0):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(self.seq_len)], dim=1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression(num_voxels_list, out_features=hidden_dim)\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model)\n",
    "\n",
    "from functools import partial\n",
    "from diffusers.models.vae import Decoder\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, h=4096, in_dim=15724, out_dim=768, seq_len=1, n_blocks=n_blocks, drop=.15, \n",
    "                 clip_size=768):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "        \n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mixer_block1(h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mixer_block2(seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.backbone_linear = nn.Linear(h * seq_len, out_dim, bias=True) \n",
    "        if clip_scale>0:\n",
    "            self.clip_proj = self.projector(clip_size, clip_size, h=clip_size)\n",
    "            \n",
    "    def projector(self, in_dim, out_dim, h=2048):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, out_dim)\n",
    "        )\n",
    "    \n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "    \n",
    "    def mixer_block1(self, h, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            self.mlp(h, h, drop),  # Token mixing\n",
    "        )\n",
    "\n",
    "    def mixer_block2(self, seq_len, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(seq_len),\n",
    "            self.mlp(seq_len, seq_len, drop)  # Channel mixing\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make empty tensors\n",
    "        c,b = torch.Tensor([0.]), torch.Tensor([[0.],[0.]])\n",
    "        \n",
    "        # Mixer blocks\n",
    "        residual1 = x\n",
    "        residual2 = x.permute(0,2,1)\n",
    "        for block1, block2 in zip(self.mixer_blocks1,self.mixer_blocks2):\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        backbone = self.backbone_linear(x).reshape(len(x), -1, self.clip_size)\n",
    "        if clip_scale>0:\n",
    "            c = self.clip_proj(backbone)\n",
    "        \n",
    "        return backbone, c, b\n",
    "\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=1, \n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 52\n",
    "    heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = VersatileDiffusionPriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "\n",
    "    model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "    )\n",
    "    \n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e720891-6575-4711-bc35-876527ed3b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---loading /weka/proj-fmri/paulscotti/MindEyeV2/train_logs/sub-001_bs24_4gpu/last.pth ckpt---\n",
      "\n",
      "[2024-08-07 12:47:24,185] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "ckpt loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model ckpt\n",
    "tag='last'\n",
    "outdir = os.path.abspath(f'../../train_logs/{model_name}')\n",
    "print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "checkpoint = torch.load(outdir+f'/{tag}.pth', map_location='cpu')\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "del checkpoint\n",
    "print(\"ckpt loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f726f617-39f5-49e2-8d0c-d11d27d01c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized embedder #0: FrozenOpenCLIPImageEmbedder with 1909889025 params. Trainable: False\n",
      "Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "vector_suffix torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "# prep unCLIP\n",
    "config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "unclip_params = config[\"model\"][\"params\"]\n",
    "network_config = unclip_params[\"network_config\"]\n",
    "denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "sampler_config = unclip_params[\"sampler_config\"]\n",
    "scale_factor = unclip_params[\"scale_factor\"]\n",
    "disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "diffusion_engine = DiffusionEngine(network_config=network_config,\n",
    "                       denoiser_config=denoiser_config,\n",
    "                       first_stage_config=first_stage_config,\n",
    "                       conditioner_config=conditioner_config,\n",
    "                       sampler_config=sampler_config,\n",
    "                       scale_factor=scale_factor,\n",
    "                       disable_first_stage_autocast=disable_first_stage_autocast)\n",
    "# set to inference\n",
    "diffusion_engine.eval().requires_grad_(False)\n",
    "diffusion_engine.to(device)\n",
    "\n",
    "ckpt_path = '/weka/proj-medarc/shared/mindeyev2_dataset/unclip6_epoch0_step110000.ckpt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "batch={\"jpg\": torch.randn(1,3,1,1).to(device), # jpg doesnt get used, it's just a placeholder\n",
    "      \"original_size_as_tuple\": torch.ones(1, 2).to(device) * 768,\n",
    "      \"crop_coords_top_left\": torch.zeros(1, 2).to(device)}\n",
    "out = diffusion_engine.conditioner(batch)\n",
    "vector_suffix = out[\"vector\"].to(device)\n",
    "print(\"vector_suffix\", vector_suffix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68abd440-7e6b-4023-9dc8-05b1b5c0baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup text caption networks\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from modeling_git import GitForCausalLMClipEmb\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-large-coco\")\n",
    "clip_text_model = GitForCausalLMClipEmb.from_pretrained(\"microsoft/git-large-coco\")\n",
    "clip_text_model.to(device) # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "clip_text_model.eval().requires_grad_(False)\n",
    "clip_text_seq_dim = 257\n",
    "clip_text_emb_dim = 1024\n",
    "\n",
    "class CLIPConverter(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIPConverter, self).__init__()\n",
    "        self.linear1 = nn.Linear(clip_seq_dim, clip_text_seq_dim)\n",
    "        self.linear2 = nn.Linear(clip_emb_dim, clip_text_emb_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x.permute(0,2,1))\n",
    "        return x\n",
    "        \n",
    "clip_convert = CLIPConverter()\n",
    "state_dict = torch.load(\"../bigG_to_L_epoch8.pth\", map_location='cpu')['model_state_dict']\n",
    "clip_convert.load_state_dict(state_dict, strict=True)\n",
    "clip_convert.to(device) # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "del state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6a706a3-d151-4643-bb34-7d08aa7361c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52360f118f04e11974d46a9c68be9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a snowboarder is skiing down a hill.', 'a man is holding a camera.', 'a man on a surfboard in the water.', 'a man on a surfboard in the water.', 'a man is sitting down and looking at the camera.', 'a plate with a plate with food on it', 'a man standing in front of a building.', 'a large blue sky.', 'a large body of water.', 'a man riding a bike on a dirt road.', 'a man riding a surfboard on top of a wave.', 'a man standing on a sidewalk next to a fence.', 'a man standing on top of a surfboard.', 'a plane is flying in the air.', 'a man standing on a snow covered slope.', 'a car driving down a street.', 'a giraffe standing next to a tree.', 'a baseball player is standing on a field.', 'a man standing next to a building.', 'a white wall', 'a kitchen with a lot of furniture.', 'a man standing on a surfboard next to a surfboard.', 'a building with a clock on it', 'a skier is skiing down a hill.', 'a man on a surfboard in the water.', 'a snowboarder is riding down a hill.', 'a dog is walking on a sidewalk.', 'a table with a plate of food on it', 'a dog is standing in the grass.', 'a man on a ski slope.', 'a plane is flying in the air.', 'a large building with a lot of windows.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-paulscotti/mindeye/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/admin/home-paulscotti/mindeye/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      " 20%|████████▊                                   | 1/5 [02:59<11:57, 179.47s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdaeb78a7174516b2544af4e262e73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a bathroom with a toilet and a sink.', 'a man riding a skateboard on top of a snow covered slope.', 'a snowboarder is skiing down a hill.', 'a plate of food', 'a street sign and a street sign', 'a zebra standing in a field.', 'a skateboarder is riding down a sidewalk.', 'a baseball player is standing on a field.', 'a view of a room.', 'a train is parked on the side of the road.', 'a large area of grass.', 'a street with a car and a bus', 'a man sitting down next to a table.', 'a man standing on a tennis court.', \"a young girl is holding a small child's hand.\", 'a man standing on a skateboard.', 'a toilet with a lid open.', 'a clock on a building.', 'a group of people sitting around a table.', 'a man riding a surfboard on top of a wave.', 'a zebra standing in a field.', 'a room with a view', 'a baseball player is standing on a field.', 'a herd of cattle grazing on a lush green field.', 'a woman standing in front of a table.', 'a large body of water', 'a table with a plate of food on it', 'a woman standing in front of a table.', 'a bus driving down a street.', 'a plane is flying in the sky.', 'a white wall', 'a cat sitting on a table.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████▌                          | 2/5 [05:59<08:59, 179.69s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9238517324407288ecba9970ae73e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a man on a beach with a surfboard.', 'a man standing next to a table.', 'a large building with a lot of windows.', 'a man sitting down next to a table.', 'a man riding a skateboard down a snow covered slope.', 'a large room with a lot of furniture.', 'a bathroom with a toilet and a sink.', 'a street sign with a sign on it.', 'a large group of people.', 'a man standing next to a building.', 'a counter with a lot of items on it', 'a man riding a skateboard on top of a sidewalk.', 'a bus driving down a street.', 'a plate with a plate with food on it', 'a street with a car and a street with a lot of cars.', 'a large boat on a lake.', 'a man standing next to a woman.', 'a herd of cattle grazing on a field.', 'a dog is standing in the grass.', 'a large room with a lot of furniture.', 'a plate of food with a knife.', 'a kitchen with a counter top and a sink in the background.', 'a man on a snowboard in a field.', 'a street with a lot of cars and a building.', 'a man and a woman are sitting together.', 'a car driving down a street next to a traffic light.', 'a view of a room.', 'a skier is skiing down a hill.', 'a bathroom with a toilet and a sink.', 'a group of people standing around each other.', 'a man is wearing a suit and tie.', 'a giraffe standing in a field.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████▍                 | 3/5 [08:57<05:57, 178.88s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b69c0a2c984a2082108905a797ca36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a person sitting down on a table.', 'a bathroom with a toilet and a sink.', 'a plate with a plate with a plate with a plate with a plate with a plate with a', 'a man standing on a field.', 'a group of people standing around.', 'a large building with a lot of windows.', 'a table with a plate and a plate on it', 'a surfer on a surfboard.', 'a man standing on top of a field.', 'a woman standing in front of a building.', 'a large elephant in a pond.', 'a man on a beach with a surfboard.', 'a man standing on a field.', 'a large body of water', 'a building with a clock on it.', 'a young man is standing in front of a camera.', 'a zebra standing on a grass covered field.', 'a street with a lot of traffic.', 'a street with a lot of cars and people.', 'a man sitting down next to a woman.', 'a plane is flying in the air.', 'a street with a car and a bus.', 'a skier is skiing on a snowy slope.', 'a room with a view', 'a baseball player is standing on a field.', 'a table with a vase and a plant on it.', 'a child sitting on a chair next to a table.', 'a woman sitting in front of a table.', 'a room with a view', 'a bathroom with a toilet and a sink.', 'a man sitting down next to a table.', 'a dog walking on a sidewalk.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████▏        | 4/5 [11:56<02:59, 179.22s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a83cfb9ae84d0681554e85f906f88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a man standing on a field.', 'a large room with a lot of furniture.', 'a plate of food with a bunch of food on it.', 'a plate with a piece of food on it', 'a plate of food', 'a bus driving down a street.', 'a young man is holding a small child.', 'a plate of food with a fork.', 'a man standing on a tennis court.', 'a woman holding a fork.', 'a man standing next to a woman.', 'a table with a plate and a plate with a plate with a plate with a plate with a', 'a young man is holding a cell phone.', 'a young man holding a tennis racket.', 'a herd of zebra grazing on a field.', 'a train is driving past a train.', 'a sign that says \" stop \" on it.', 'a group of people playing a game of tennis.', 'a white and black wall', 'a bathroom with a toilet and a sink.', 'a plate of food with a fork.', 'a man is holding a phone.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [14:05<00:00, 169.03s/it]\n",
      "/admin/home-paulscotti/mindeye/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved sub-001_bs24_4gpu outputs!\n"
     ]
    }
   ],
   "source": [
    "# get all reconstructions\n",
    "model.to(device)\n",
    "model.eval().requires_grad_(False)\n",
    "\n",
    "all_blurryrecons = None\n",
    "all_images = None\n",
    "all_recons = None\n",
    "all_predcaptions = []\n",
    "all_clipvoxels = None\n",
    "all_prior_out = None\n",
    "all_backbones = None\n",
    "\n",
    "minibatch_size = 32\n",
    "num_samples_per_image = 1\n",
    "plotting = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(range(0,len(test_images),minibatch_size)):\n",
    "        start_time = time.time() \n",
    "\n",
    "        image = test_images[batch:batch+minibatch_size]\n",
    "        voxel = test_voxels[batch:batch+minibatch_size].unsqueeze(1).to(device)\n",
    "\n",
    "        # Save ground truth images\n",
    "        if all_images is None:\n",
    "            all_images = image\n",
    "        else:\n",
    "            all_images = torch.vstack((all_images, image))\n",
    "        \n",
    "        voxel_ridge = model.ridge(voxel,0)\n",
    "        backbone, clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge)\n",
    "                \n",
    "        # Save retrieval submodule outputs\n",
    "        if clip_scale>0:\n",
    "            if all_clipvoxels is None:\n",
    "                all_clipvoxels = clip_voxels.cpu()\n",
    "            else:\n",
    "                all_clipvoxels = torch.vstack((all_clipvoxels, clip_voxels.cpu()))\n",
    "                \n",
    "        # Feed voxels through OpenCLIP-bigG diffusion prior\n",
    "        prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, \n",
    "                        text_cond = dict(text_embed = backbone), \n",
    "                        cond_scale = 1., timesteps = 20).cpu()\n",
    "        \n",
    "        if all_prior_out is None:\n",
    "            all_prior_out = prior_out\n",
    "        else:\n",
    "            all_prior_out = torch.vstack((all_prior_out, prior_out))\n",
    "\n",
    "        pred_caption_emb = clip_convert(prior_out.to(device).float())\n",
    "        generated_ids = clip_text_model.generate(pixel_values=pred_caption_emb, max_length=20)\n",
    "        generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_predcaptions = np.hstack((all_predcaptions, generated_caption))\n",
    "        print(generated_caption)\n",
    "        \n",
    "        # Feed diffusion prior outputs through unCLIP\n",
    "        if plotting:\n",
    "            jj=-1\n",
    "            fig, axes = plt.subplots(1, 12, figsize=(10, 4))\n",
    "\n",
    "        for i in range(len(voxel)):\n",
    "            samples = utils.unclip_recon(prior_out[[i]],\n",
    "                             diffusion_engine,\n",
    "                             vector_suffix,\n",
    "                             num_samples=num_samples_per_image)\n",
    "            if all_recons is None:\n",
    "                all_recons = samples.cpu()\n",
    "            else:\n",
    "                all_recons = torch.vstack((all_recons, samples.cpu()))\n",
    "                \n",
    "            if plotting:  \n",
    "                jj+=1\n",
    "                axes[jj].imshow(utils.torch_to_Image(image[i]))\n",
    "                axes[jj].axis('off')\n",
    "                jj+=1\n",
    "                axes[jj].imshow(utils.torch_to_Image(samples.cpu()[0]))\n",
    "                axes[jj].axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        print(model_name)\n",
    "        err # dont actually want to run the whole thing with plotting=True\n",
    "\n",
    "# resize outputs before saving\n",
    "imsize = 256\n",
    "all_images = transforms.Resize((imsize,imsize))(all_images).float()\n",
    "all_recons = transforms.Resize((imsize,imsize))(all_recons).float()\n",
    "if blurry_recon: \n",
    "    all_blurryrecons = transforms.Resize((imsize,imsize))(all_blurryrecons).float()\n",
    "        \n",
    "## Saving ##\n",
    "if \"MST\" in model_name:\n",
    "    np.save(f\"evals/{model_name}/{model_name}_MST_ID.npy\", MST_ID)\n",
    "torch.save(all_images.cpu(),f\"evals/{model_name}/{model_name}_all_images.pt\")\n",
    "torch.save(all_recons,f\"evals/{model_name}/{model_name}_all_recons.pt\")\n",
    "if clip_scale>0:\n",
    "    torch.save(all_clipvoxels,f\"evals/{model_name}/{model_name}_all_clipvoxels.pt\")\n",
    "torch.save(all_prior_out,f\"evals/{model_name}/{model_name}_all_prior_out.pt\")\n",
    "torch.save(all_predcaptions,f\"evals/{model_name}/{model_name}_all_predcaptions.pt\")\n",
    "print(f\"saved {model_name} outputs!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindeye",
   "language": "python",
   "name": "mindeye"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
