{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30bdc821",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e64e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "!module load fsl/6.0.6.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "import pandas as pd\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder # bigG embedder\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d758c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env = os.environ.copy()\n",
    "my_env[\"PATH\"] = \"/home/rk1593/.conda/envs/rt_mindEye2/bin:\" + my_env[\"PATH\"]\n",
    "os.environ.update(my_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc5d2e32-6027-4a19-bef4-5ca068db35bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n"
     ]
    }
   ],
   "source": [
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "\n",
    "# First use \"accelerate config\" in terminal and setup using deepspeed stage 2 with CPU offloading!\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "if utils.is_interactive(): # set batch size here if using interactive notebook instead of submitting job\n",
    "    global_batch_size = batch_size = 8\n",
    "else:\n",
    "    global_batch_size = os.environ[\"GLOBAL_BATCH_SIZE\"]\n",
    "    batch_size = int(os.environ[\"GLOBAL_BATCH_SIZE\"]) // num_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b767ab6f-d4a9-47a5-b3bf-f56bf6760c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 485757\n",
      "device: cuda\n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: testing\n",
      "--data_path=/scratch/gpfs/rk1593/rt_mindEye                     --cache_dir=/scratch/gpfs/rk1593/rt_mindEye/cache                     --model_name=multisubject_subj01_1024hid_nolow_300ep                     --no-multi_subject --no-visualize_prior --subj=1 --batch_size=64 --num_sessions=1                     --hidden_dim=1024 --clip_scale=1.                     --no-blurry_recon --blur_scale=.5                      --seq_past=0 --seq_future=0                     --use_prior --prior_scale=30 --multisubject_ckpt=/scratch/gpfs/rk1593/rt_mindEye/multisubject_subj01_1024hid_nolow_300ep                     --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=300 --no-use_image_aug                     --ckpt_interval=50 --ckpt_saving --wandb_log\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"testing\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the above cells\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=/scratch/gpfs/rk1593/rt_mindEye \\\n",
    "                    --cache_dir=/scratch/gpfs/rk1593/rt_mindEye/cache \\\n",
    "                    --model_name=multisubject_subj01_1024hid_nolow_300ep \\\n",
    "                    --no-multi_subject --no-visualize_prior --subj=1 --batch_size=64 --num_sessions=1 \\\n",
    "                    --hidden_dim=1024 --clip_scale=1. \\\n",
    "                    --no-blurry_recon --blur_scale=.5  \\\n",
    "                    --seq_past=0 --seq_future=0 \\\n",
    "                    --use_prior --prior_scale=30 --multisubject_ckpt=/scratch/gpfs/rk1593/rt_mindEye/multisubject_subj01_1024hid_nolow_300ep \\\n",
    "                    --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=300 --no-use_image_aug \\\n",
    "                    --ckpt_interval=50 --ckpt_saving --wandb_log\"# \\\n",
    "                    #--multisubject_ckpt=../train_logs/multisubject_subj01_acc\"\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [1] num_sessions 1\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multisubject_ckpt\", type=str, default=None,\n",
    "    help=\"Path to pre-trained multisubject model to finetune a single subject from. multisubject must be False.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=0,\n",
    "    help=\"Number of training sessions to include (zero = all sessions)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visualize_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"output visualizations from unCLIP every ckpt_interval (requires more memory!)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=16,\n",
    "    help=\"Batch size can be increased by 10x if only training retreival submodule and not diffusion prior\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=.5,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=30,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=150,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1024,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_past\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_future\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'/scratch/gpfs/rk1593/rt_mindEye/train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug or blurry_recon:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "if use_image_aug:\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    \n",
    "if multi_subject:\n",
    "    subj_list = np.arange(1,9)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "source": [
    "### Creating wds dataloader, preload betas and all 73k possible images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aefe7c27-ab39-4b2c-90f4-480f4087b7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "batch_size = 64 num_iterations_per_epoch = 11 num_samples_per_epoch = 750\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "num_voxels_list = []\n",
    "\n",
    "if multi_subject:\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "    num_samples_per_epoch = (750*40) // num_devices \n",
    "else:\n",
    "    num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81084834-035f-4465-ad59-59e6b806a2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 1 sessions\n",
      "/scratch/gpfs/rk1593/rt_mindEye/wds/subj01/train/0.tar\n",
      "np.shape(betas):  (30000, 15724)\n",
      "num_voxels for subj01: 15724\n",
      "Loaded all subj train dls and betas!\n",
      "\n",
      "currently using 1 seq_len (chose 0 past behav and 0 future behav)\n"
     ]
    }
   ],
   "source": [
    "train_data = {}\n",
    "train_dl = {}\n",
    "num_voxels = {}\n",
    "voxels = {}\n",
    "for s in subj_list:\n",
    "    print(f\"Training with {num_sessions} sessions\")\n",
    "    if multi_subject:\n",
    "        train_url = f\"{data_path}/wds/subj0{s}/train/0.tar\"\n",
    "    else:\n",
    "        train_url = f\"{data_path}/wds/subj0{s}/train/0.tar\"\n",
    "    print(train_url)\n",
    "\n",
    "    train_data[f'subj0{s}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=my_split_by_node)\\\n",
    "                        .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                        .decode(\"torch\")\\\n",
    "                        .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                        .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "    train_dl[f'subj0{s}'] = torch.utils.data.DataLoader(train_data[f'subj0{s}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "    f = h5py.File(f'{data_path}/betas_all_subj0{s}_fp32_renorm.hdf5', 'r')\n",
    "    betas = f['betas'][:]\n",
    "    print(\"np.shape(betas): \", np.shape(betas))\n",
    "    betas = torch.Tensor(betas).to(\"cpu\").to(data_type)\n",
    "    num_voxels_list.append(betas[0].shape[-1])\n",
    "    num_voxels[f'subj0{s}'] = betas[0].shape[-1]\n",
    "    voxels[f'subj0{s}'] = betas\n",
    "    print(f\"num_voxels for subj0{s}: {num_voxels[f'subj0{s}']}\")\n",
    "\n",
    "print(\"Loaded all subj train dls and betas!\\n\")\n",
    "\n",
    "\n",
    "seq_len = seq_past + 1 + seq_future\n",
    "print(f\"currently using {seq_len} seq_len (chose {seq_past} past behav and {seq_future} future behav)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674446e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36aa4d95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab0ca9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_custom = np.zeros(shape=(30000, 15724))\n",
    "import os\n",
    "import nibabel as nib\n",
    "from subprocess import call\n",
    "from datetime import datetime, date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba262a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73bd19f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 104, 83)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_img = nib.load('/scratch/gpfs/rk1593/rt_mindEye/raw/subj01/nsdgeneral.nii.gz')\n",
    "mask_np = mask_img.get_fdata()\n",
    "mask_np.shape\n",
    "#np.unique(mask_np, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d2b6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ref vol\n",
    "# from nilearn.image import get_data, index_img, concat_imgs, new_img_like\n",
    "\n",
    "# ndscore_bold_img = nib.load(f'/scratch/gpfs/rk1593/rt_mindEye/raw/subj01/raw/0/sub-01_ses-nsd01_task-nsdcore_run-01_bold.nii.gz')\n",
    "# refvol = ndscore_bold_img.get_fdata()[:,:,:,:10]\n",
    "# refvol = np.mean(refvol,axis = 3)\n",
    "# refvol = new_img_like(ref_niimg = ndscore_bold_img, data = refvol,copy_header=True)\n",
    "# nib.save(refvol,'/scratch/gpfs/rk1593/rt_mindEye/preprocessing_intermediates/refvol.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d7f616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e5edf2a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1be6181",
   "metadata": {
    "tags": []
   },
   "source": [
    "we need fsl, talk to della people\n",
    "\n",
    "tr 1 is 0 to 1.6\n",
    "\n",
    "run mcflirt on whole brain so .par will give you the tr by 6 and that will be the confounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d1d61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d4 = Apr-18-2024\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_485757/2333577167.py:72: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'COCOimage_46003.0_trialNum_1.0_runNum0' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  new_events_df.loc[i_trial, \"trial_type\"] = \"COCOimage_\" + str(trial[\"73k_id\"]) + \"_trialNum_\" + str(trial[\"trial_number\"]) + \"_runNum\" + str(run_num)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "(188, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rk1593/.conda/envs/rt_mindEye2/lib/python3.11/site-packages/nilearn/glm/first_level/experimental_paradigm.py:167: UserWarning: The following unexpected columns in events data will be ignored: trial_number, response_time, response, missing_data, is_old_session, time, is_correct_session, memory_first, 10k_id, is_correct, total_old_presses, 73k_id, total_novel_presses, change_mind, memory_recent\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving...\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_485757/2333577167.py:72: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'COCOimage_4997.0_trialNum_1.0_runNum1' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  new_events_df.loc[i_trial, \"trial_type\"] = \"COCOimage_\" + str(trial[\"73k_id\"]) + \"_trialNum_\" + str(trial[\"trial_number\"]) + \"_runNum\" + str(run_num)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "(188, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rk1593/.conda/envs/rt_mindEye2/lib/python3.11/site-packages/nilearn/glm/first_level/experimental_paradigm.py:167: UserWarning: The following unexpected columns in events data will be ignored: trial_number, response_time, response, missing_data, is_old_session, time, is_correct_session, memory_first, 10k_id, is_correct, total_old_presses, 73k_id, total_novel_presses, change_mind, memory_recent\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving...\n",
      "2\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_485757/2333577167.py:72: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'COCOimage_4777.0_trialNum_1.0_runNum2' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  new_events_df.loc[i_trial, \"trial_type\"] = \"COCOimage_\" + str(trial[\"73k_id\"]) + \"_trialNum_\" + str(trial[\"trial_number\"]) + \"_runNum\" + str(run_num)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "(188, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rk1593/.conda/envs/rt_mindEye2/lib/python3.11/site-packages/nilearn/glm/first_level/experimental_paradigm.py:167: UserWarning: The following unexpected columns in events data will be ignored: trial_number, response_time, response, missing_data, is_old_session, time, is_correct_session, memory_first, 10k_id, is_correct, total_old_presses, 73k_id, total_novel_presses, change_mind, memory_recent\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving...\n",
      "3\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_485757/2333577167.py:72: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'COCOimage_30374.0_trialNum_1.0_runNum3' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  new_events_df.loc[i_trial, \"trial_type\"] = \"COCOimage_\" + str(trial[\"73k_id\"]) + \"_trialNum_\" + str(trial[\"trial_number\"]) + \"_runNum\" + str(run_num)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "betas_custom = np.zeros(shape=(30000, 15724))\n",
    "import os\n",
    "import nibabel as nib\n",
    "from subprocess import call\n",
    "from datetime import datetime, date\n",
    "from nilearn.glm.first_level import *\n",
    "from nilearn import image, plotting\n",
    "import nibabel as nib\n",
    "\n",
    "from subprocess import call\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date\n",
    "from nilearn.image import get_data, index_img, concat_imgs, new_img_like\n",
    "from nilearn.signal import clean\n",
    "range_runs = (1,13)\n",
    "# load in the bolds imgs time series and events tsv from nsd rawdata\n",
    "ndscore_bold_imgs = [nib.load(f'/scratch/gpfs/rk1593/rt_mindEye/raw/subj01/raw/0/sub-01_ses-nsd01_task-nsdcore_run-{run:02d}_bold.nii.gz') for run in range(range_runs[0], range_runs[1])]\n",
    "ndscore_events = [pd.read_csv(f'/scratch/gpfs/rk1593/rt_mindEye/raw/subj01/raw/0/sub-01_ses-nsd01_task-nsdcore_run-{run:02d}_events.tsv', sep = \"\\t\", header = 0) for run in range(range_runs[0], range_runs[1])]\n",
    "# create a new list of events_df's which will have the trial_type modified to be unique identifiers\n",
    "new_ndscore_events = []\n",
    "mask_img = nib.load('/scratch/gpfs/rk1593/rt_mindEye/masking_operation/thresholded_masks/day1_subj1/mask_transformed_0.4444440000_day1_subj1.nii.gz')\n",
    "mask_on = True\n",
    "if not mask_on:\n",
    "    mask_img = None\n",
    "lsa_glm = FirstLevelModel(t_r=1.6,slice_time_ref=.5,hrf_model='glover',\n",
    "                        drift_model=None,high_pass=None,mask_img=mask_img,\n",
    "                        signal_scaling=False,smoothing_fwhm=None,noise_model='ar1',\n",
    "                        n_jobs=-1,verbose=-1,memory_level=1,minimize_memory=True)\n",
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "# Month abbreviation, day and year\t\n",
    "d4 = today.strftime(\"%b-%d-%Y\")\n",
    "print(\"d4 =\", d4)\n",
    "\n",
    "def motion_correct(in_=None, reffile=None, out=None):\n",
    "    # Motion correct to this run's functional reference\n",
    "    # takes short 10 volume bold run and temporal avg of that as single 3d reference volume\n",
    "    # create pseudo\n",
    "    \n",
    "    command = f\"mcflirt -in {in_} -reffile {reffile} -plots -out {out}\"\n",
    "    A = datetime.now().timestamp(); call(command,shell=True); B = datetime.now().timestamp()\n",
    "    pad_print(f\"Motion correction time: {B-A:.4f}\")\n",
    "!module load fsl/6.0.6.2 \n",
    "\n",
    "# this for loop is going to give us beta maps with their trial names\n",
    "# we have a beta map for each stimulus trial given in all 12 runs of day 1 here\n",
    "beta_maps_list = []\n",
    "all_trial_names_list = []\n",
    "starting_run = 0\n",
    "if starting_run > 0:\n",
    "    if mask_on:\n",
    "        beta_maps_list = np.load(\"/scratch/gpfs/rk1593/rt_mindEye/masked_betas_subj1_day1_up_to_run\" + \\\n",
    "                                 str(starting_run - 1) + \"_\" + d4 + \".npy\").tolist()\n",
    "        all_trial_names_list = np.load(\"/scratch/gpfs/rk1593/rt_mindEye/masked_trial_ids_up_to_run\" + \\\n",
    "                                       str(starting_run - 1) + \"_\" + d4 + \".npy\").tolist()\n",
    "\n",
    "    else:\n",
    "        beta_maps_list = np.load(\"/scratch/gpfs/rk1593/rt_mindEye/betas_subj1_day1_up_to_run\" + \\\n",
    "                                 str(starting_run - 1) + \"_\" + d4 + \".npy\").tolist()\n",
    "        all_trial_names_list = np.load(\"/scratch/gpfs/rk1593/rt_mindEye/trial_ids_up_to_run\" + \\\n",
    "                                       str(starting_run - 1) + \"_\" + d4 + \".npy\").tolist()\n",
    "\n",
    "\n",
    "for run_num,events_df in enumerate(ndscore_events):\n",
    "    print(run_num)\n",
    "    if run_num < starting_run:\n",
    "        continue\n",
    "    new_events_df = events_df\n",
    "    for i_trial, trial in new_events_df.iterrows():\n",
    "        new_events_df.loc[i_trial, \"trial_type\"] = \"COCOimage_\" + str(trial[\"73k_id\"]) + \"_trialNum_\" + str(trial[\"trial_number\"]) + \"_runNum\" + str(run_num)\n",
    "    mc_params = []\n",
    "    imgs = []\n",
    "    raw = ndscore_bold_imgs[run_num]\n",
    "    ntrs = raw.shape[-1]\n",
    "    for tr in range(ntrs):\n",
    "        print(tr)\n",
    "        tmp = '/scratch/gpfs/rk1593/rt_mindEye/preprocessing_intermediates/tmp.nii'\n",
    "        nib.save(index_img(raw,tr),tmp)\n",
    "\n",
    "        # motion correct and load in the parameters\n",
    "        mc = '/scratch/gpfs/rk1593/rt_mindEye/preprocessing_intermediates/tmp_mc'\n",
    "        os.system(f\"mcflirt -in {tmp} -reffile /scratch/gpfs/rk1593/rt_mindEye/preprocessing_intermediates/refvol.nii -plots -out {mc}\")\n",
    "        mc_params.append(np.loadtxt(f'{mc}.par'))\n",
    "\n",
    "        #spatial smoothing\n",
    "        sm = '/scratch/gpfs/rk1593/rt_mindEye/preprocessing_intermediates/tmp_mc_sm'\n",
    "        os.system(f'fslmaths {mc} -kernel gauss {5/2.3548} -fmean {sm}')\n",
    "        imgs.append(get_data(sm + \".nii.gz\"))\n",
    "    \n",
    "    img = np.rollaxis(np.array(imgs),0,4)\n",
    "    img = new_img_like(raw,img,copy_header=True)\n",
    "#     out = f'/scratch/gpfs/rk1593/rt_mindEye/preprocessing_intermediates/sub-01_ses-nsd01_task-nsdcore_run-{run_num:02d}_bold_mc_sm.nii.gz'\n",
    "#     nib.save(img,out)\n",
    "    mc_params = np.array(mc_params)\n",
    "    print(mc_params.shape)\n",
    "    lsa_glm.fit(run_imgs=img,events=new_events_df, confounds = pd.DataFrame(mc_params))\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    plotting.plot_design_matrix(lsa_glm.design_matrices_[0], ax=ax)\n",
    "    fig.show()\n",
    "    trialwise_conditions = new_events_df[\"trial_type\"].unique()\n",
    "    for condition in trialwise_conditions:\n",
    "        beta_map = lsa_glm.compute_contrast(str(condition), output_type=\"effect_size\")\n",
    "        beta_maps_list.append(beta_map.get_fdata().flatten().tolist())\n",
    "        all_trial_names_list.append(condition)\n",
    "    print(\"saving...\")\n",
    "    if mask_on:\n",
    "        np.save(file = \"/scratch/gpfs/rk1593/rt_mindEye/masked_betas_subj1_day1_up_to_run\" + str(run_num) + \"_\" + d4, arr = np.vstack(beta_maps_list))\n",
    "        np.save(file = \"/scratch/gpfs/rk1593/rt_mindEye/masked_trial_ids_up_to_run\" + str(run_num) + \"_\" + d4, arr = np.array(all_trial_names_list))\n",
    "\n",
    "    else:\n",
    "        np.save(file = \"/scratch/gpfs/rk1593/rt_mindEye/betas_subj1_day1_up_to_run\" + str(run_num) + \"_\" + d4, arr = np.vstack(beta_maps_list))\n",
    "        np.save(file = \"/scratch/gpfs/rk1593/rt_mindEye/trial_ids_up_to_run\" + str(run_num) + \"_\" + d4, arr = np.array(all_trial_names_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "359cab59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>-0.007908</td>\n",
       "      <td>-0.035572</td>\n",
       "      <td>-0.027762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.027855</td>\n",
       "      <td>-0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000375</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.016802</td>\n",
       "      <td>-0.008750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.076325</td>\n",
       "      <td>0.003737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000317</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000205</td>\n",
       "      <td>-0.005581</td>\n",
       "      <td>0.048545</td>\n",
       "      <td>0.004360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>-0.008011</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>-0.003602</td>\n",
       "      <td>-0.219049</td>\n",
       "      <td>0.211424</td>\n",
       "      <td>0.098236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>-0.007802</td>\n",
       "      <td>0.005377</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>-0.213965</td>\n",
       "      <td>0.176155</td>\n",
       "      <td>0.087738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>-0.008363</td>\n",
       "      <td>0.005457</td>\n",
       "      <td>-0.003659</td>\n",
       "      <td>-0.222344</td>\n",
       "      <td>0.229931</td>\n",
       "      <td>0.085833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>-0.008037</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>-0.003758</td>\n",
       "      <td>-0.223556</td>\n",
       "      <td>0.180027</td>\n",
       "      <td>0.075353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>-0.007745</td>\n",
       "      <td>0.005428</td>\n",
       "      <td>-0.003829</td>\n",
       "      <td>-0.219642</td>\n",
       "      <td>0.126038</td>\n",
       "      <td>0.097109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5\n",
       "0    0.000000 -0.000219  0.000057 -0.007908 -0.035572 -0.027762\n",
       "1    0.000380  0.000092  0.000000  0.000002  0.027855 -0.000016\n",
       "2   -0.000375 -0.000000 -0.000160 -0.000009  0.016802 -0.008750\n",
       "3    0.000053 -0.000000 -0.000165  0.000010 -0.076325  0.003737\n",
       "4   -0.000317 -0.000000 -0.000205 -0.005581  0.048545  0.004360\n",
       "..        ...       ...       ...       ...       ...       ...\n",
       "183 -0.008011  0.005528 -0.003602 -0.219049  0.211424  0.098236\n",
       "184 -0.007802  0.005377 -0.003540 -0.213965  0.176155  0.087738\n",
       "185 -0.008363  0.005457 -0.003659 -0.222344  0.229931  0.085833\n",
       "186 -0.008037  0.005609 -0.003758 -0.223556  0.180027  0.075353\n",
       "187 -0.007745  0.005428 -0.003829 -0.219642  0.126038  0.097109\n",
       "\n",
       "[188 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(mc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6b8b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rk1593/.conda/envs/rt_mindEye2/lib/python3.11/site-packages/nilearn/glm/first_level/experimental_paradigm.py:167: UserWarning: The following unexpected columns in events data will be ignored: is_correct_session, total_old_presses, 73k_id, 10k_id, missing_data, total_novel_presses, time, is_correct, change_mind, response_time, memory_first, response, trial_number, is_old_session, memory_recent\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>FirstLevelModel(drift_model=None, high_pass=None, n_jobs=-1,\n",
       "                signal_scaling=False, slice_time_ref=0.5, t_r=1.6, verbose=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;FirstLevelModel<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>FirstLevelModel(drift_model=None, high_pass=None, n_jobs=-1,\n",
       "                signal_scaling=False, slice_time_ref=0.5, t_r=1.6, verbose=-1)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "FirstLevelModel(drift_model=None, high_pass=None, n_jobs=-1,\n",
       "                signal_scaling=False, slice_time_ref=0.5, t_r=1.6, verbose=-1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.rollaxis(np.array(imgs),0,4)\n",
    "img = new_img_like(raw,img,copy_header=True)\n",
    "lsa_glm.fit(run_imgs=img,events=new_events_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a304813b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_map.get_fdata().flatten().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4db4960c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 COCOimage_15493.0_trialNum_1.0_runNum4\n",
      "1 COCOimage_70130.0_trialNum_2.0_runNum4\n",
      "2 COCOimage_28681.0_trialNum_3.0_runNum4\n",
      "3 COCOimage_60313.0_trialNum_4.0_runNum4\n",
      "4 COCOimage_8384.0_trialNum_5.0_runNum4\n",
      "5 COCOimage_25288.0_trialNum_6.0_runNum4\n",
      "6 COCOimage_63815.0_trialNum_7.0_runNum4\n",
      "7 COCOimage_35920.0_trialNum_8.0_runNum4\n",
      "8 COCOimage_49454.0_trialNum_9.0_runNum4\n",
      "9 COCOimage_8390.0_trialNum_10.0_runNum4\n",
      "10 COCOimage_46020.0_trialNum_11.0_runNum4\n",
      "11 COCOimage_1290.0_trialNum_12.0_runNum4\n",
      "12 COCOimage_47426.0_trialNum_13.0_runNum4\n",
      "13 COCOimage_16764.0_trialNum_14.0_runNum4\n",
      "14 COCOimage_21602.0_trialNum_15.0_runNum4\n",
      "15 COCOimage_70125.0_trialNum_16.0_runNum4\n",
      "16 COCOimage_45129.0_trialNum_17.0_runNum4\n",
      "17 COCOimage_6559.0_trialNum_18.0_runNum4\n",
      "18 COCOimage_37593.0_trialNum_19.0_runNum4\n",
      "19 COCOimage_66692.0_trialNum_20.0_runNum4\n",
      "20 COCOimage_63122.0_trialNum_21.0_runNum4\n",
      "21 COCOimage_68716.0_trialNum_22.0_runNum4\n",
      "22 COCOimage_69035.0_trialNum_23.0_runNum4\n",
      "23 COCOimage_52372.0_trialNum_24.0_runNum4\n",
      "24 COCOimage_47401.0_trialNum_25.0_runNum4\n",
      "25 COCOimage_19948.0_trialNum_26.0_runNum4\n",
      "26 COCOimage_16549.0_trialNum_27.0_runNum4\n",
      "27 COCOimage_41709.0_trialNum_28.0_runNum4\n",
      "28 COCOimage_54927.0_trialNum_29.0_runNum4\n",
      "29 COCOimage_58350.0_trialNum_30.0_runNum4\n",
      "30 COCOimage_70336.0_trialNum_31.0_runNum4\n",
      "31 COCOimage_16724.0_trialNum_32.0_runNum4\n",
      "32 COCOimage_70328.0_trialNum_33.0_runNum4\n",
      "33 COCOimage_19948.0_trialNum_34.0_runNum4\n",
      "34 COCOimage_61948.0_trialNum_35.0_runNum4\n",
      "35 COCOimage_15523.0_trialNum_36.0_runNum4\n",
      "36 COCOimage_23642.0_trialNum_37.0_runNum4\n",
      "37 COCOimage_23642.0_trialNum_38.0_runNum4\n",
      "38 COCOimage_33484.0_trialNum_39.0_runNum4\n",
      "39 COCOimage_2738.0_trialNum_40.0_runNum4\n",
      "40 COCOimage_48203.0_trialNum_41.0_runNum4\n",
      "41 COCOimage_11783.0_trialNum_42.0_runNum4\n",
      "42 COCOimage_15239.0_trialNum_43.0_runNum4\n",
      "43 COCOimage_6117.0_trialNum_44.0_runNum4\n",
      "44 COCOimage_32142.0_trialNum_45.0_runNum4\n",
      "45 COCOimage_47762.0_trialNum_46.0_runNum4\n",
      "46 COCOimage_38041.0_trialNum_47.0_runNum4\n",
      "47 COCOimage_43859.0_trialNum_48.0_runNum4\n",
      "48 COCOimage_57751.0_trialNum_49.0_runNum4\n",
      "49 COCOimage_70955.0_trialNum_50.0_runNum4\n",
      "50 COCOimage_35853.0_trialNum_51.0_runNum4\n",
      "51 COCOimage_13001.0_trialNum_52.0_runNum4\n",
      "52 COCOimage_52604.0_trialNum_53.0_runNum4\n",
      "53 COCOimage_52652.0_trialNum_54.0_runNum4\n",
      "54 COCOimage_23497.0_trialNum_55.0_runNum4\n",
      "55 COCOimage_40576.0_trialNum_56.0_runNum4\n",
      "56 COCOimage_32143.0_trialNum_57.0_runNum4\n",
      "57 COCOimage_46858.0_trialNum_58.0_runNum4\n",
      "58 COCOimage_18557.0_trialNum_59.0_runNum4\n",
      "59 COCOimage_7025.0_trialNum_60.0_runNum4\n",
      "60 COCOimage_50103.0_trialNum_61.0_runNum4\n",
      "61 COCOimage_15492.0_trialNum_62.0_runNum4\n",
      "62 COCOimage_22925.0_trialNum_63.0_runNum4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     beta_maps_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(beta_maps_list, beta_map\u001b[38;5;241m.\u001b[39mget_fdata()\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m      6\u001b[0m     all_trial_names_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(all_trial_names_list, condition)\n\u001b[0;32m----> 7\u001b[0m np\u001b[38;5;241m.\u001b[39msave(file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/scratch/gpfs/rk1593/rt_mindEye/betas_subj1_day1_up_to_run\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(run_num), arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta_maps_list\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m np\u001b[38;5;241m.\u001b[39msave(file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/scratch/gpfs/rk1593/rt_mindEye/trial_ids_up_to_run\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(run_num), arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_trial_names_list))\n",
      "File \u001b[0;32m~/.conda/envs/rt_mindEye2/lib/python3.11/site-packages/numpy/core/shape_base.py:286\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_vhstack_dispatcher)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvstack\u001b[39m(tup, \u001b[38;5;241m*\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    Stack arrays in sequence vertically (row wise).\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m \u001b[43matleast_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    288\u001b[0m         arrs \u001b[38;5;241m=\u001b[39m [arrs]\n",
      "File \u001b[0;32m~/.conda/envs/rt_mindEye2/lib/python3.11/site-packages/numpy/core/shape_base.py:81\u001b[0m, in \u001b[0;36matleast_2d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_atleast_2d_dispatcher\u001b[39m(\u001b[38;5;241m*\u001b[39marys):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arys\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_atleast_2d_dispatcher)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21matleast_2d\u001b[39m(\u001b[38;5;241m*\u001b[39marys):\n\u001b[1;32m     83\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    View inputs as arrays with at least two dimensions.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     res \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trialwise_conditions = new_events_df[\"trial_type\"].unique()\n",
    "for index,condition in enumerate(trialwise_conditions):\n",
    "    print(index, condition)\n",
    "    beta_map = lsa_glm.compute_contrast(str(condition), output_type=\"effect_size\")\n",
    "    beta_maps_list = np.append(beta_maps_list, beta_map.get_fdata().flatten())\n",
    "    all_trial_names_list = np.append(all_trial_names_list, condition)\n",
    "np.save(file = \"/scratch/gpfs/rk1593/rt_mindEye/betas_subj1_day1_up_to_run\" + str(run_num), arr = np.vstack(beta_maps_list))\n",
    "np.save(file = \"/scratch/gpfs/rk1593/rt_mindEye/trial_ids_up_to_run\" + str(run_num), arr = np.array(all_trial_names_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(file = \"/scratch/gpfs/rk1593/rt_mindEye/betas_subj1_day1_up_to_run\" + str(run_num), arr = np.vstack(beta_maps_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:] #[:1000] # if you go OOM you can remove the [:] so it isnt preloaded to cpu! (will require a few edits elsewhere tho)\n",
    "images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images to cpu!\", images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62d9fd97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(487468800,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_maps_list.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "source": [
    "### CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "#     arch=\"ViT-bigG-14\",\n",
    "#     version=\"laion2b_s39b_b160k\",\n",
    "#     output_tokens=True,\n",
    "#     only_tokens=True\n",
    "# )\n",
    "import pickle\n",
    "with open(\"/scratch/gpfs/rk1593/rt_mindEye/clip_img_embedder\", \"rb\") as input_file:\n",
    "    clip_img_embedder = pickle.load(input_file)\n",
    "clip_img_embedder.to(device)\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a27903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b79bd38-6990-4504-8d45-4a68d57d8885",
   "metadata": {},
   "source": [
    "### SD VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01baff79-8114-482b-b115-6f05aa8ad691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if blurry_recon:\n",
    "    from diffusers import AutoencoderKL    \n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=256,\n",
    "    )\n",
    "    ckpt = torch.load(f'{cache_dir}/sd_image_var_autoenc.pth')\n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    \n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)\n",
    "    \n",
    "    from autoencoder.convnext import ConvnextXL\n",
    "    cnx = ConvnextXL(f'{cache_dir}/convnext_xlarge_alpha0.75_fullckpt.pth')\n",
    "    cnx.requires_grad_(False)\n",
    "    cnx.eval()\n",
    "    cnx.to(device)\n",
    "    \n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).to(device).reshape(1,3,1,1)\n",
    "    std = torch.tensor([0.228, 0.224, 0.225]).to(device).reshape(1,3,1,1)\n",
    "    \n",
    "    blur_augs = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.8),\n",
    "        kornia.augmentation.RandomGrayscale(p=0.1),\n",
    "        kornia.augmentation.RandomSolarize(p=0.1),\n",
    "        kornia.augmentation.RandomResizedCrop((224,224), scale=(.9,.9), ratio=(1,1), p=1.0),\n",
    "        data_keys=[\"input\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e5e4a-f697-4b2c-88fc-01f6a54886c0",
   "metadata": {},
   "source": [
    "### MindEye modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c271b-173f-472e-b059-a2eda0f4c4c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a5d61-4769-40b9-a004-f4e7b5b38bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(seq_len)], dim=1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression(num_voxels_list, out_features=hidden_dim, seq_len=seq_len)\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test on subject 1 with fake data\n",
    "b = torch.randn((2,seq_len,num_voxels_list[0]))\n",
    "print(b.shape, model.ridge(b,0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8de65a-6d3b-4248-bea9-9b6f4d562321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models import BrainNetwork\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, n_blocks=n_blocks,\n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim, \n",
    "                          blurry_recon=blurry_recon, clip_scale=clip_scale)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works on some fake data\n",
    "b = torch.randn((2,seq_len,hidden_dim))\n",
    "print(\"b.shape\",b.shape)\n",
    "\n",
    "backbone_, clip_, blur_ = model.backbone(b)\n",
    "print(backbone_.shape, clip_.shape, blur_[0].shape, blur_[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397c0d7-52a3-4153-823b-c27d2eb3eeba",
   "metadata": {},
   "source": [
    "### Adding diffusion prior + unCLIP if use_prior=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69965344-9346-4592-9cc5-e537e31d5fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 52\n",
    "    heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = PriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "\n",
    "    model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "    )\n",
    "    \n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)\n",
    "    \n",
    "    # prep unCLIP\n",
    "    if visualize_prior:\n",
    "        from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "        from omegaconf import OmegaConf\n",
    "        \n",
    "        config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "        config = OmegaConf.to_container(config, resolve=True)\n",
    "        unclip_params = config[\"model\"][\"params\"]\n",
    "        network_config = unclip_params[\"network_config\"]\n",
    "        denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "        first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "        conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "        sampler_config = unclip_params[\"sampler_config\"]\n",
    "        scale_factor = unclip_params[\"scale_factor\"]\n",
    "        disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "        offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "        first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "        sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "        diffusion_engine = DiffusionEngine(network_config=network_config,\n",
    "                               denoiser_config=denoiser_config,\n",
    "                               first_stage_config=first_stage_config,\n",
    "                               conditioner_config=conditioner_config,\n",
    "                               sampler_config=sampler_config,\n",
    "                               scale_factor=scale_factor,\n",
    "                               disable_first_stage_autocast=disable_first_stage_autocast)\n",
    "        # set to inference\n",
    "        diffusion_engine.eval().requires_grad_(False)\n",
    "        diffusion_engine.to(device)\n",
    "\n",
    "        ckpt_path = f'{cache_dir}/unclip6_epoch0_step110000.ckpt'\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "        diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "        image = images[:1].to(device)\n",
    "        batch={\"jpg\": image,\n",
    "              \"original_size_as_tuple\": torch.ones(image.shape[0], 2).to(device) * 768,\n",
    "              \"crop_coords_top_left\": torch.zeros(image.shape[0], 2).to(device)}\n",
    "        out = diffusion_engine.conditioner(batch)\n",
    "        vector_suffix = out[\"vector\"].to(device)\n",
    "        print(\"vector_suffix\", vector_suffix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25271a-2209-400c-8026-df3b8ddc1eef",
   "metadata": {},
   "source": [
    "### Setup optimizer / lr / ckpt saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "if use_prior:\n",
    "    opt_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ])\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir + \"/\" + tag\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,outdir=outdir,multisubj_loading=False): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "        state_dict.pop('ridge.linears.0.weight',None)\n",
    "    model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_epoch:\n",
    "        globals()[\"epoch\"] = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "    if load_optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if load_lr:\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    del checkpoint\n",
    "        \n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_log = False\n",
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'mindeye'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "      \"num_params\": num_params,\n",
    "      \"clip_scale\": clip_scale,\n",
    "      \"prior_scale\": prior_scale,\n",
    "      \"blur_scale\": blur_scale,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"num_test\": num_test,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f43a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "multisubject_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e8770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'/scratch/gpfs/rk1593/rt_mindEye/multisubject_subj01_1024hid_nolow_300ep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba080c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load multisubject stage1 ckpt if set\n",
    "if multisubject_ckpt is not None and not resume_from_ckpt:\n",
    "    load_ckpt(\"100_epochs_fine_tune_subj1day1Mar-21-2024.pth\",outdir=multisubject_ckpt,load_lr=False,load_optimizer=False,load_epoch=False,strict=False,multisubj_loading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f3de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5453c316-0cb0-4bee-8585-f44dff746e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved ckpt model weights into current model\n",
    "if resume_from_ckpt:\n",
    "    load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)\n",
    "elif wandb_log:\n",
    "    if wandb.run.resumed:\n",
    "        load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f09f76-4481-4133-b09a-a22b10dbc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dls = [train_dl[f'subj0{s}'] for s in subj_list]\n",
    "\n",
    "model, optimizer, *train_dls, lr_scheduler = accelerator.prepare(model, optimizer, *train_dls, lr_scheduler)\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be0d5f-3e94-4612-9373-61b53d836393",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, train_dl in enumerate(train_dls):\n",
    "    print(s)\n",
    "    with torch.cuda.amp.autocast(dtype=data_type):\n",
    "        for iter, (behav0, past_behav0, future_behav0, old_behav0) in enumerate(train_dl):  \n",
    "             pass\n",
    "           # print(iter, behav0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a502b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "behav0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae8d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "test_image, test_voxel = None, None\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "datetime_today4 = today.strftime(\"%b-%d-%Y\")\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    \n",
    "    recon_cossim = 0.\n",
    "    test_recon_cossim = 0.\n",
    "    recon_mse = 0.\n",
    "    test_recon_mse = 0.\n",
    "\n",
    "    loss_clip_total = 0.\n",
    "    loss_blurry_total = 0.\n",
    "    loss_blurry_cont_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "    \n",
    "    loss_prior_total = 0.\n",
    "    test_loss_prior_total = 0.\n",
    "\n",
    "    blurry_pixcorr = 0.\n",
    "    test_blurry_pixcorr = 0. # needs >.456 to beat low-level subj01 results in mindeye v1\n",
    "\n",
    "    # pre-load all batches for this epoch (it's MUCH faster to pre-load in bulk than to separate loading per batch)\n",
    "    voxel_iters = {} # empty dict because diff subjects have differing # of voxels\n",
    "    image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 3, 224, 224).float()\n",
    "    annot_iters = {}\n",
    "    perm_iters, betas_iters, select_iters = {}, {}, {}\n",
    "    for s, train_dl in enumerate(train_dls):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            for iter, (behav0, past_behav0, future_behav0, old_behav0) in enumerate(train_dl):  \n",
    "          \n",
    "                image0 = images[behav0[:,0,0].cpu().long()].float()\n",
    "                image_iters[iter,s*batch_size:s*batch_size+batch_size] = image0\n",
    "                print(\"behav0[:,0,5].cpu().long(): \", behav0[:,0,5].cpu().long())\n",
    "                voxel0 = voxels[f'subj0{subj_list[s]}'][behav0[:,0,5].cpu().long()]\n",
    "                # ipdb> voxel0.shape => torch.Size([64, 15724]), get this 64 trials out of the 30K with the beta matrix for those trials\n",
    "                # so we would want to feed in each run as training data\n",
    "                # we wouold want to read in all the runs from day 1 and create a np.zeros(30K,15724)matrix but then\n",
    "                # populate the day 1 trials with the correct beta matrices\n",
    "                # \n",
    "                import pdb \n",
    "                pdb.set_trace()\n",
    "                voxel0 = torch.Tensor(voxel0)\n",
    "                voxel0 = voxel0.unsqueeze(1)\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    voxel0, perm, betas, select = utils.mixco(voxel0)\n",
    "                    perm_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = perm\n",
    "                    betas_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = betas\n",
    "                    select_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = select\n",
    "\n",
    "                voxel_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = voxel0\n",
    "\n",
    "                if iter >= num_iterations_per_epoch-1:\n",
    "                    break\n",
    "\n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    for train_i in range(num_iterations_per_epoch):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss=0.\n",
    "\n",
    "            voxel_list = [voxel_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "            image = image_iters[train_i].detach()\n",
    "            image = image.to(device)\n",
    "\n",
    "            if use_image_aug: \n",
    "                image = img_augment(image)\n",
    "\n",
    "            clip_target = clip_img_embedder(image)\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                perm_list = [perm_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                perm = torch.cat(perm_list, dim=0)\n",
    "                betas_list = [betas_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                betas = torch.cat(betas_list, dim=0)\n",
    "                select_list = [select_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                select = torch.cat(select_list, dim=0)\n",
    "\n",
    "            voxel_ridge_list = [model.ridge(voxel_list[si],si) for si,s in enumerate(subj_list)]\n",
    "            voxel_ridge = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "            backbone, clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge)\n",
    "\n",
    "            if clip_scale>0:\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "            if use_prior:\n",
    "                loss_prior, prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                loss_prior_total += loss_prior.item()\n",
    "                loss_prior *= prior_scale\n",
    "                loss += loss_prior\n",
    "\n",
    "                recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                recon_mse += mse(prior_out, clip_target).item()\n",
    "\n",
    "            if clip_scale>0:\n",
    "                if epoch < int(mixup_pct * num_epochs):                \n",
    "                    loss_clip = utils.mixco_nce(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006,\n",
    "                        perm=perm, betas=betas, select=select)\n",
    "                else:\n",
    "                    epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=epoch_temp)\n",
    "\n",
    "                loss_clip_total += loss_clip.item()\n",
    "                loss_clip *= clip_scale\n",
    "                loss += loss_clip\n",
    "\n",
    "            if blurry_recon:     \n",
    "                image_enc_pred, transformer_feats = blurry_image_enc_\n",
    "\n",
    "                image_enc = autoenc.encode(2*image-1).latent_dist.mode() * 0.18215\n",
    "                loss_blurry = l1(image_enc_pred, image_enc)\n",
    "                loss_blurry_total += loss_blurry.item()\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    image_enc_shuf = image_enc[perm]\n",
    "                    betas_shape = [-1] + [1]*(len(image_enc.shape)-1)\n",
    "                    image_enc[select] = image_enc[select] * betas[select].reshape(*betas_shape) + \\\n",
    "                        image_enc_shuf[select] * (1 - betas[select]).reshape(*betas_shape)\n",
    "\n",
    "                image_norm = (image - mean)/std\n",
    "                image_aug = (blur_augs(image) - mean)/std\n",
    "                _, cnx_embeds = cnx(image_norm)\n",
    "                _, cnx_aug_embeds = cnx(image_aug)\n",
    "\n",
    "                cont_loss = utils.soft_cont_loss(\n",
    "                    nn.functional.normalize(transformer_feats.reshape(-1, transformer_feats.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_aug_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    temp=0.2)\n",
    "                loss_blurry_cont_total += cont_loss.item()\n",
    "\n",
    "                loss += (loss_blurry + 0.1*cont_loss) * blur_scale #/.18215\n",
    "\n",
    "            if clip_scale>0:\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "            if blurry_recon:\n",
    "                with torch.no_grad():\n",
    "                    # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                    random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                    blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "            utils.check_loss(loss)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "\n",
    "            # if finished training, save jpg recons if they exist\n",
    "            if (epoch == num_epochs-1) or (epoch % ckpt_interval == 0):\n",
    "                if blurry_recon:    \n",
    "                    image_enc = autoenc.encode(2*image[:4]-1).latent_dist.mode() * 0.18215\n",
    "                    # transform blurry recon latents to images and plot it\n",
    "                    fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                    jj=-1\n",
    "                    for j in [0,1,2,3]:\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc_pred[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "\n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/blur_recons\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                        plt.close()\n",
    "                    else:\n",
    "                        plt.show()\n",
    "                        \n",
    "                if use_prior and visualize_prior: # output recons every ckpt\n",
    "                    idx = np.random.randint(0, 3)\n",
    "                    print(f\"reconstructing... idx={idx}\")\n",
    "                    samples = utils.unclip_recon(prior_out[[idx]],\n",
    "                             diffusion_engine,\n",
    "                             vector_suffix)\n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/orig\"] = wandb.Image(transforms.ToPILImage()(image[idx]),\n",
    "                                                           caption=f\"epoch{epoch:03d}\")\n",
    "                        logs[f\"test/recons\"] = wandb.Image(transforms.ToPILImage()(samples[0]),\n",
    "                                                           caption=f\"epoch{epoch:03d}\")\n",
    "                    if utils.is_interactive():\n",
    "                        plt.figure(figsize=(2,2))\n",
    "                        plt.imshow(transforms.ToPILImage()(image[idx]))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "                        \n",
    "                        plt.figure(figsize=(2,2))\n",
    "                        plt.imshow(transforms.ToPILImage()(samples[0]))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "\n",
    "          \n",
    "\n",
    "            \n",
    "    # Save model checkpoint and reconstruct\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "         save_ckpt(str(epoch) + \"_epochs_fine_tune_subj1day1\" + datetime_today4 + \".pth\")\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if ckpt_saving:\n",
    "    save_ckpt(str(epoch) +\"_epochs_fine_tune_subj1day1\" + datetime_today4 + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e81ae3-171f-40ad-a3e8-24bee4472325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b429dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64129b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de8a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd87fe9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799ffe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4846eada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0cbda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rt_mindEye2 [~/.conda/envs/rt_mindEye2/]",
   "language": "python",
   "name": "conda_rt_mindeye2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
